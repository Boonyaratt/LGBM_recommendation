{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Boonyaratt/Two-Tower-Recommendation/blob/master/Promotion_LGBMRanker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c4d1f4e",
      "metadata": {
        "id": "7c4d1f4e"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.Import Library & Datasets"
      ],
      "metadata": {
        "id": "s7Vz4fVba0A2"
      },
      "id": "s7Vz4fVba0A2"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3c3af875",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c3af875",
        "outputId": "8e7b775c-3bd2-4962-9684-f96bde210344"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pandas version: 2.2.2\n",
            "NumPy version: 2.0.2\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "from dateutil import easter\n",
        "from typing import Dict, Text\n",
        "import os, kagglehub\n",
        "import lightgbm as lgb\n",
        "import heapq\n",
        "from functools import lru_cache\n",
        "from collections import defaultdict\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Pandas version:\", pd.__version__)\n",
        "print(\"NumPy version:\", np.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "x8MG3DQOYu9e",
      "metadata": {
        "id": "x8MG3DQOYu9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db8dce3c-19db-45fc-a882-dd10bf90c1d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘Dataset’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets"
      ],
      "metadata": {
        "id": "VACBZDvta4V-"
      },
      "id": "VACBZDvta4V-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Display Dataset Information"
      ],
      "metadata": {
        "id": "L_b6bUrua4-Q"
      },
      "id": "L_b6bUrua4-Q"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "vHjw9JF0ZX3W",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHjw9JF0ZX3W",
        "outputId": "b91e431d-ed4f-4e31-e9e7-ccbb798ae6be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'dunnhumby-the-complete-journey' dataset.\n",
            "/kaggle/input/dunnhumby-the-complete-journey\n"
          ]
        }
      ],
      "source": [
        "import os, pathlib\n",
        "os.environ[\"KAGGLEHUB_CACHE\"] = \"/content/Dataset\"  # exact case\n",
        "pathlib.Path(\"/content/Dataset\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "import kagglehub\n",
        "kaggle_path = kagglehub.dataset_download(\"frtgnn/dunnhumby-the-complete-journey\")\n",
        "print(kaggle_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d29b67bc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d29b67bc",
        "outputId": "20af20fd-9932-49d0-d47b-186e11fe3270"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrames loaded successfully!\n",
            "campaign_desc shape: (30, 4)\n",
            "campaign_table shape: (7208, 3)\n",
            "coupon_redempt shape: (2318, 4)\n",
            "coupon shape: (124548, 3)\n",
            "product shape: (92353, 7)\n",
            "transaction_data shape: (2595732, 12)\n",
            "Headers for each dataset:\n",
            "==================================================\n",
            "\n",
            "campaign_desc:\n",
            "Columns: ['DESCRIPTION', 'CAMPAIGN', 'START_DAY', 'END_DAY']\n",
            "\n",
            "campaign_table:\n",
            "Columns: ['DESCRIPTION', 'household_key', 'CAMPAIGN']\n",
            "\n",
            "coupon_redempt:\n",
            "Columns: ['household_key', 'DAY', 'COUPON_UPC', 'CAMPAIGN']\n",
            "\n",
            "coupon:\n",
            "Columns: ['COUPON_UPC', 'PRODUCT_ID', 'CAMPAIGN']\n",
            "\n",
            "product:\n",
            "Columns: ['PRODUCT_ID', 'MANUFACTURER', 'DEPARTMENT', 'BRAND', 'COMMODITY_DESC', 'SUB_COMMODITY_DESC', 'CURR_SIZE_OF_PRODUCT']\n",
            "\n",
            "transaction_data:\n",
            "Columns: ['household_key', 'BASKET_ID', 'DAY', 'PRODUCT_ID', 'QUANTITY', 'SALES_VALUE', 'STORE_ID', 'RETAIL_DISC', 'TRANS_TIME', 'WEEK_NO', 'COUPON_DISC', 'COUPON_MATCH_DISC']\n"
          ]
        }
      ],
      "source": [
        "# Read all CSV files\n",
        "\n",
        "# path = \"/kaggle/input/dunnhumby-the-complete-journey/\"\n",
        "path = \"/content/Dataset/datasets/frtgnn/dunnhumby-the-complete-journey/versions/1/\"\n",
        "\n",
        "campaign_desc = pd.read_csv(path + \"campaign_desc.csv\")\n",
        "campaign_table = pd.read_csv(path + \"campaign_table.csv\")\n",
        "coupon_redempt = pd.read_csv(path + \"coupon_redempt.csv\") ## 1 counpon_unc can have multiple product_id: 556 nunique from 2318\n",
        "coupon = pd.read_csv(path + \"coupon.csv\")\n",
        "product = pd.read_csv(path + \"product.csv\")\n",
        "transaction_data = pd.read_csv(path + \"transaction_data.csv\")\n",
        "# Check the dataframes\n",
        "print(\"DataFrames loaded successfully!\")\n",
        "print(f\"campaign_desc shape: {campaign_desc.shape}\")\n",
        "print(f\"campaign_table shape: {campaign_table.shape}\")\n",
        "print(f\"coupon_redempt shape: {coupon_redempt.shape}\")\n",
        "print(f\"coupon shape: {coupon.shape}\")\n",
        "print(f\"product shape: {product.shape}\")\n",
        "print(f\"transaction_data shape: {transaction_data.shape}\")\n",
        "\n",
        "# Show headers for each dataset\n",
        "print(\"Headers for each dataset:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "datasets = {\n",
        "    'campaign_desc': campaign_desc,\n",
        "    'campaign_table': campaign_table,\n",
        "    'coupon_redempt': coupon_redempt,\n",
        "    'coupon': coupon,\n",
        "    'product': product,\n",
        "    'transaction_data': transaction_data\n",
        "}\n",
        "\n",
        "for name, df in datasets.items():\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"Columns: {list(df.columns)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "product.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4rsfstKp3KZ",
        "outputId": "b83246ab-bb07-423c-ee56-55566be05694"
      },
      "id": "-4rsfstKp3KZ",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 92353 entries, 0 to 92352\n",
            "Data columns (total 7 columns):\n",
            " #   Column                Non-Null Count  Dtype \n",
            "---  ------                --------------  ----- \n",
            " 0   PRODUCT_ID            92353 non-null  int64 \n",
            " 1   MANUFACTURER          92353 non-null  int64 \n",
            " 2   DEPARTMENT            92353 non-null  object\n",
            " 3   BRAND                 92353 non-null  object\n",
            " 4   COMMODITY_DESC        92353 non-null  object\n",
            " 5   SUB_COMMODITY_DESC    92353 non-null  object\n",
            " 6   CURR_SIZE_OF_PRODUCT  92353 non-null  object\n",
            "dtypes: int64(2), object(5)\n",
            "memory usage: 4.9+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2-1. Sample Selection"
      ],
      "metadata": {
        "id": "Nz9dqfm-Eiw7"
      },
      "id": "Nz9dqfm-Eiw7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Houeholds Key Selection"
      ],
      "metadata": {
        "id": "PgGUZmU0Eqcq"
      },
      "id": "PgGUZmU0Eqcq"
    },
    {
      "cell_type": "code",
      "source": [
        "# keep top-N households by txn count (สำหรับเทสต์)\n",
        "N_HH = 500\n",
        "top_hh = (transaction_data[\"household_key\"].value_counts().head(N_HH).index.astype(str))\n",
        "transaction_data = transaction_data[transaction_data[\"household_key\"].astype(str).isin(top_hh)].copy()\n",
        "campaign_table   = campaign_table[campaign_table[\"household_key\"].astype(str).isin(set(top_hh))].copy()\n",
        "coupon_redempt   = coupon_redempt[coupon_redempt[\"household_key\"].astype(str).isin(set(top_hh))].copy()"
      ],
      "metadata": {
        "id": "70d9dYTbxEwR"
      },
      "id": "70d9dYTbxEwR",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Window day Selection"
      ],
      "metadata": {
        "id": "WeYFBPkbExcY"
      },
      "id": "WeYFBPkbExcY"
    },
    {
      "cell_type": "code",
      "source": [
        "# === OPTIONAL: Dataset windowing to speed up ===\n",
        "# ใช้ช่วงวันล่าสุด N วัน (เช่น 180 วันล่าสุด) หรือกำหนดช่วงเองก็ได้\n",
        "WINDOW_DAYS = 180\n",
        "\n",
        "min_day_all = int(transaction_data[\"DAY\"].min())\n",
        "max_day_all = int(transaction_data[\"DAY\"].max())\n",
        "start_day = max(min_day_all, max_day_all - WINDOW_DAYS + 1)\n",
        "end_day   = max_day_all\n",
        "\n",
        "# 1) ตัดธุรกรรมให้เหลือเฉพาะช่วงวัน\n",
        "transaction_data = transaction_data[(transaction_data[\"DAY\"] >= start_day) & (transaction_data[\"DAY\"] <= end_day)].copy()\n",
        "\n",
        "# 2) จำกัด household ให้เหลือเฉพาะที่ยังมีธุรกรรมในช่วงวัน\n",
        "keep_hh = set(transaction_data[\"household_key\"].astype(str).unique())\n",
        "campaign_table = campaign_table[campaign_table[\"household_key\"].astype(str).isin(keep_hh)].copy()\n",
        "coupon_redempt = coupon_redempt[coupon_redempt[\"household_key\"].astype(str).isin(keep_hh)].copy()\n",
        "\n",
        "# 3) จำกัดแคมเปญให้ทับซ้อนช่วงวัน (START_DAY..END_DAY) กับหน้าต่างที่เลือก\n",
        "campaign_desc = campaign_desc[\n",
        "    (campaign_desc[\"END_DAY\"] >= start_day) & (campaign_desc[\"START_DAY\"] <= end_day)\n",
        "].copy()\n",
        "keep_campaigns = set(campaign_desc[\"CAMPAIGN\"].astype(str).unique())\n",
        "campaign_table = campaign_table[campaign_table[\"CAMPAIGN\"].astype(str).isin(keep_campaigns)].copy()\n",
        "coupon_redempt = coupon_redempt[coupon_redempt[\"CAMPAIGN\"].astype(str).isin(keep_campaigns)].copy()\n",
        "coupon = coupon[coupon[\"CAMPAIGN\"].astype(str).isin(keep_campaigns)].copy()\n",
        "\n",
        "# 4) (ออปชัน) จำกัดสินค้าให้เหลือเฉพาะที่ปรากฏในธุรกรรมช่วงนี้ เพื่อยุบมิติ OHE\n",
        "keep_products = set(transaction_data[\"PRODUCT_ID\"].astype(str).unique())\n",
        "product = product[product[\"PRODUCT_ID\"].astype(str).isin(keep_products)].copy()\n",
        "\n",
        "print(f\"Using DAY window [{start_day}, {end_day}]\")\n",
        "print(\"transaction_data:\", transaction_data.shape, \"campaign_desc:\", campaign_desc.shape,\n",
        "      \"campaign_table:\", campaign_table.shape, \"coupon_redempt:\", coupon_redempt.shape,\n",
        "      \"coupon:\", coupon.shape, \"product:\", product.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k37NQZlexCZh",
        "outputId": "1eef0f7c-90a6-4ef5-c5d4-37d53f56fcaa"
      },
      "id": "k37NQZlexCZh",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using DAY window [532, 711]\n",
            "transaction_data: (367796, 12) campaign_desc: (13, 4) campaign_table: (2168, 3) coupon_redempt: (1177, 4) coupon: (87484, 3) product: (36206, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transaction_data['household_key'] = transaction_data['household_key'].astype(str)\n",
        "transaction_data['STORE_ID'] = transaction_data['STORE_ID'].astype(str)\n",
        "transaction_data['PRODUCT_ID'] = transaction_data['PRODUCT_ID'].astype(str)\n",
        "transaction_data['BASKET_ID'] = transaction_data['BASKET_ID'].astype(str)\n",
        "\n",
        "# FIXED: Use .astype(str) instead of str() for the entire column\n",
        "coupon['COUPON_UPC'] = coupon['COUPON_UPC'].astype(str)  # Changed this line\n",
        "coupon['PRODUCT_ID'] = coupon['PRODUCT_ID'].astype(str)\n",
        "coupon['CAMPAIGN'] = coupon['CAMPAIGN'].astype(str)\n",
        "\n",
        "campaign_desc['CAMPAIGN'] = campaign_desc['CAMPAIGN'].astype(str)\n",
        "\n",
        "coupon_redempt['household_key'] = coupon_redempt['household_key'].astype(str)\n",
        "coupon_redempt['COUPON_UPC'] = coupon_redempt['COUPON_UPC'].astype(str)\n",
        "coupon_redempt['CAMPAIGN'] = coupon_redempt['CAMPAIGN'].astype(str)\n",
        "\n",
        "campaign_table['CAMPAIGN'] = campaign_table['CAMPAIGN'].astype(str)\n",
        "campaign_table['household_key'] = campaign_table['household_key'].astype(str)\n",
        "\n",
        "product['PRODUCT_ID'] = product['PRODUCT_ID'].astype(str)\n",
        "product['DEPARTMENT'] = product['DEPARTMENT'].astype(str)\n",
        "product['MANUFACTURER'] = product['MANUFACTURER'].astype(str)"
      ],
      "metadata": {
        "id": "vmlp7kXlASd8"
      },
      "id": "vmlp7kXlASd8",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.Data Preparation"
      ],
      "metadata": {
        "id": "TeBa4UNqbkII"
      },
      "id": "TeBa4UNqbkII"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3-1. Merging"
      ],
      "metadata": {
        "id": "Q23mi2mrfatH"
      },
      "id": "Q23mi2mrfatH"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define promotion key\n",
        "coupon['COUPON_UPC'] = coupon['COUPON_UPC'].astype(str)\n",
        "coupon['CAMPAIGN'] = coupon['CAMPAIGN'].astype(str)\n",
        "campaign_desc['CAMPAIGN'] = campaign_desc['CAMPAIGN'].astype(str)\n",
        "coupon_redempt['COUPON_UPC'] = coupon_redempt['COUPON_UPC'].astype(str)\n",
        "coupon_redempt['CAMPAIGN'] = coupon_redempt['CAMPAIGN'].astype(str)\n",
        "\n",
        "coupon['PROMO_KEY'] = coupon['COUPON_UPC'] + '|' + coupon['CAMPAIGN']\n",
        "coupon_redempt['PROMO_KEY'] = coupon_redempt['COUPON_UPC'] + '|' + coupon_redempt['CAMPAIGN']\n",
        "\n",
        "promo_catalog = coupon.merge(\n",
        "    campaign_desc[['CAMPAIGN','START_DAY','END_DAY']],\n",
        "    on='CAMPAIGN', how='left'\n",
        ")[['PROMO_KEY','COUPON_UPC','CAMPAIGN','PRODUCT_ID','START_DAY','END_DAY']]\n",
        "\n",
        "# All campaigns per household\n",
        "hh_campaigns = campaign_table[['household_key','CAMPAIGN']].drop_duplicates()\n",
        "hh_promos = hh_campaigns.merge(\n",
        "    promo_catalog[['CAMPAIGN','PROMO_KEY','START_DAY','END_DAY']],\n",
        "    on='CAMPAIGN', how='left'\n",
        ").dropna(subset=['PROMO_KEY'])\n",
        "\n",
        "# Promo -> set of products\n",
        "promo_to_products = (\n",
        "    coupon[['COUPON_UPC','CAMPAIGN','PRODUCT_ID']]\n",
        "    .assign(PROMO_KEY=lambda df: df['COUPON_UPC'] + '|' + df['CAMPAIGN'])\n",
        "    .groupby('PROMO_KEY')['PRODUCT_ID']\n",
        "    .apply(lambda s: set(s.astype(str)))\n",
        "    .to_dict()\n",
        ")\n",
        "\n",
        "# One-hot for promo metadata aggregated from product\n",
        "coupon_prod = coupon.merge(\n",
        "    product[['PRODUCT_ID','DEPARTMENT','BRAND']],\n",
        "    on='PRODUCT_ID', how='left'\n",
        ").assign(PROMO_KEY=lambda df: df['COUPON_UPC'] + '|' + df['CAMPAIGN'])\n",
        "coupon_prod['DEPARTMENT'] = coupon_prod['DEPARTMENT'].fillna('UNK')\n",
        "coupon_prod['BRAND'] = coupon_prod['BRAND'].fillna('UNK')\n",
        "promo_ohe = pd.get_dummies(\n",
        "    coupon_prod[['PROMO_KEY','DEPARTMENT','BRAND']], columns=['DEPARTMENT','BRAND'], drop_first=False\n",
        ").groupby('PROMO_KEY').max().astype(bool)\n",
        "\n",
        "# Promo popularity by redemptions\n",
        "promo_pop = coupon_redempt['PROMO_KEY'].value_counts().to_dict()\n",
        "\n",
        "# Redemptions per (household, DAY) as positives lookup\n",
        "redempt_by_hh_day = (\n",
        "    coupon_redempt.groupby(['household_key','DAY'])['PROMO_KEY']\n",
        "    .apply(set).to_dict()\n",
        ")"
      ],
      "metadata": {
        "id": "sf01JeobcVq_"
      },
      "id": "sf01JeobcVq_",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "coupon_redempt= coupon_redempt.merge(\n",
        "    coupon[['COUPON_UPC', 'PRODUCT_ID']],\n",
        "    on='COUPON_UPC',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "coupon= coupon.merge(\n",
        "    campaign_desc[['START_DAY', 'END_DAY', 'CAMPAIGN']],\n",
        "    on='CAMPAIGN',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "coupon = coupon.merge(\n",
        "    product[['PRODUCT_ID', 'DEPARTMENT', 'MANUFACTURER']],\n",
        "    on='PRODUCT_ID',\n",
        "    how='left'\n",
        ")\n"
      ],
      "metadata": {
        "id": "y7wKOyiJzFuL"
      },
      "id": "y7wKOyiJzFuL",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3-2. Create Time *Features*"
      ],
      "metadata": {
        "id": "IC8r_sn1V5sP"
      },
      "id": "IC8r_sn1V5sP"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "8331929f",
      "metadata": {
        "id": "8331929f"
      },
      "outputs": [],
      "source": [
        "# Create timestamp feature from DAY and TRANS_TIME\n",
        "def create_timestamp(day, trans_time):\n",
        "    base_date = datetime(2010, 3, 24)  # Arbitrary base date\n",
        "    date_part = base_date + timedelta(days=int(day) - 1)\n",
        "\n",
        "    # Convert TRANS_TIME to time\n",
        "    hours = int(trans_time) // 100\n",
        "    minutes = int(trans_time) % 100\n",
        "    time_part = timedelta(hours=hours, minutes=minutes)\n",
        "\n",
        "    return date_part + time_part\n",
        "\n",
        "# Add timestamp to transaction_data\n",
        "transaction_data['timestamp'] = transaction_data.apply(\n",
        "    lambda row: create_timestamp(row['DAY'], row['TRANS_TIME']), axis=1\n",
        ")\n",
        "\n",
        "ts = transaction_data[\"timestamp\"]\n",
        "transaction_data[\"hour\"]        = ts.dt.hour.astype(\"Int64\")\n",
        "\n",
        "transaction_data[\"dayofweek\"]   = ts.dt.dayofweek.astype(\"Int64\")    # 0=Mon .. 6=Sun\n",
        "transaction_data[\"is_weekend\"]  = transaction_data[\"dayofweek\"].isin([5,6]).astype(\"int64\")\n",
        "\n",
        "transaction_data[\"day\"]         = ts.dt.day.astype(\"Int64\")\n",
        "transaction_data[\"week\"]        = ts.dt.isocalendar().week.astype(\"Int64\")\n",
        "transaction_data[\"month\"]       = ts.dt.month.astype(\"Int64\")\n",
        "transaction_data[\"quarter\"]     = ts.dt.quarter.astype(\"Int64\")\n",
        "transaction_data[\"year\"]        = ts.dt.year.astype(\"Int64\")\n",
        "\n",
        "def make_part_of_day(hhmm_int):\n",
        "    hh = hhmm_int // 100\n",
        "    return (\n",
        "        \"night\" if hh < 6 else\n",
        "        \"morning\" if hh < 12 else\n",
        "        \"afternoon\" if hh < 18 else\n",
        "        \"evening\"\n",
        "    )\n",
        "\n",
        "transaction_data[\"part_of_day\"] = transaction_data[\"TRANS_TIME\"].astype(int).apply(make_part_of_day)\n",
        "transaction_data = transaction_data.merge(\n",
        "        product[['PRODUCT_ID', 'DEPARTMENT',\"MANUFACTURER\",\"BRAND\"]],\n",
        "        on='PRODUCT_ID',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "transaction_data = transaction_data.merge(\n",
        "    coupon_redempt,\n",
        "    on=['household_key', 'DAY', 'PRODUCT_ID'],\n",
        "    how='left'\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transaction_data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hM6Aws_EBc7-",
        "outputId": "375e8e49-2e11-4f65-caed-ddfaf59990f8"
      },
      "id": "hM6Aws_EBc7-",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 368093 entries, 0 to 368092\n",
            "Data columns (total 28 columns):\n",
            " #   Column             Non-Null Count   Dtype         \n",
            "---  ------             --------------   -----         \n",
            " 0   household_key      368093 non-null  object        \n",
            " 1   BASKET_ID          368093 non-null  object        \n",
            " 2   DAY                368093 non-null  int64         \n",
            " 3   PRODUCT_ID         368093 non-null  object        \n",
            " 4   QUANTITY           368093 non-null  int64         \n",
            " 5   SALES_VALUE        368093 non-null  float64       \n",
            " 6   STORE_ID           368093 non-null  object        \n",
            " 7   RETAIL_DISC        368093 non-null  float64       \n",
            " 8   TRANS_TIME         368093 non-null  int64         \n",
            " 9   WEEK_NO            368093 non-null  int64         \n",
            " 10  COUPON_DISC        368093 non-null  float64       \n",
            " 11  COUPON_MATCH_DISC  368093 non-null  float64       \n",
            " 12  timestamp          368093 non-null  datetime64[ns]\n",
            " 13  hour               368093 non-null  Int64         \n",
            " 14  dayofweek          368093 non-null  Int64         \n",
            " 15  is_weekend         368093 non-null  int64         \n",
            " 16  day                368093 non-null  Int64         \n",
            " 17  week               368093 non-null  Int64         \n",
            " 18  month              368093 non-null  Int64         \n",
            " 19  quarter            368093 non-null  Int64         \n",
            " 20  year               368093 non-null  Int64         \n",
            " 21  part_of_day        368093 non-null  object        \n",
            " 22  DEPARTMENT         368093 non-null  object        \n",
            " 23  MANUFACTURER       368093 non-null  object        \n",
            " 24  BRAND              368093 non-null  object        \n",
            " 25  COUPON_UPC         1830 non-null    object        \n",
            " 26  CAMPAIGN           1830 non-null    object        \n",
            " 27  PROMO_KEY          1830 non-null    object        \n",
            "dtypes: Int64(7), datetime64[ns](1), float64(4), int64(5), object(11)\n",
            "memory usage: 81.1+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-3. Global Constants"
      ],
      "metadata": {
        "id": "3KrekZ2XWbC-"
      },
      "id": "3KrekZ2XWbC-"
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_STATE = 42\n",
        "TOPK_CANDS = 50         # จำนวน candidates ต่อ query ที่จะป้อนเข้า ranker\n",
        "EVAL_AT = [5, 10]          # NDCG@5, NDCG@10"
      ],
      "metadata": {
        "id": "Zg4X_B6VBQm_"
      },
      "id": "Zg4X_B6VBQm_",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3-4. Create *Sequential Orders* per household\n",
        "\n",
        "Aggregate transactions into baskets per `(household_key, BASKET_ID)`:\n",
        "- `items`: set of `PRODUCT_ID`s in basket.\n",
        "- `basket_day`: the day of the basket.\n",
        "- `next_items` / `next_day`: next basket’s items/day (for sequence context).\n",
        "\n",
        "#### Why\n",
        "Provides user history over time for candidate generation without leakage."
      ],
      "metadata": {
        "id": "-FFqmmFvWhj0"
      },
      "id": "-FFqmmFvWhj0"
    },
    {
      "cell_type": "code",
      "source": [
        "transaction_data = transaction_data.sort_values([\"household_key\", \"timestamp\", \"BASKET_ID\"])\n",
        "orders = (\n",
        "    transaction_data.groupby([\"household_key\", \"BASKET_ID\"])\n",
        "    .agg(items=(\"PRODUCT_ID\", lambda s: set(s.tolist())),\n",
        "         basket_day=(\"DAY\", \"min\"))\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# for now 'next_items' and 'next'day' is not neccesary for training due to we use real-time suggestion not next-day suggestion\n",
        "orders[\"next_items\"] = orders.groupby(\"household_key\")[\"items\"].shift(-1)\n",
        "orders[\"next_day\"] = orders.groupby(\"household_key\")[\"basket_day\"].shift(-1)\n",
        "orders = orders.dropna(subset=[\"next_day\"]).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "BSk4OvrBATvk"
      },
      "id": "BSk4OvrBATvk",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "RLvLx53QUkZj",
        "outputId": "36723b95-cbc7-40ef-ede2-6684152c4df5"
      },
      "id": "RLvLx53QUkZj",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      household_key    BASKET_ID  \\\n",
              "0                 1  35840912229   \n",
              "1                 1  35841522378   \n",
              "2                 1  35865806127   \n",
              "3                 1  35877676640   \n",
              "4                 1  36027807102   \n",
              "...             ...          ...   \n",
              "32501           998  41918011754   \n",
              "32502           998  42031013564   \n",
              "32503           998  42049756099   \n",
              "32504           998  42063285897   \n",
              "32505           998  42185598715   \n",
              "\n",
              "                                                   items  basket_day  \\\n",
              "0      {7130924, 5577022, 995242, 1124029, 1088462, 8...         535   \n",
              "1                                               {819312}         536   \n",
              "2                                              {9527290}         537   \n",
              "3                                              {1069297}         538   \n",
              "4      {991580, 5569368, 5582712, 936685, 900875, 971...         540   \n",
              "...                                                  ...         ...   \n",
              "32501              {1091365, 1134152, 1007136, 15830211}         683   \n",
              "32502           {981760, 995242, 849843, 899624, 845208}         691   \n",
              "32503                                           {962568}         693   \n",
              "32504                         {843338, 865874, 12524086}         694   \n",
              "32505                        {1018085, 1049922, 6533765}         703   \n",
              "\n",
              "                                              next_items  next_day  \n",
              "0                                               {819312}     536.0  \n",
              "1                                              {9527290}     537.0  \n",
              "2                                              {1069297}     538.0  \n",
              "3      {991580, 5569368, 5582712, 936685, 900875, 971...     540.0  \n",
              "4                     {5577022, 1069575, 856942, 961554}     546.0  \n",
              "...                                                  ...       ...  \n",
              "32501           {981760, 995242, 849843, 899624, 845208}     691.0  \n",
              "32502                                           {962568}     693.0  \n",
              "32503                         {843338, 865874, 12524086}     694.0  \n",
              "32504                        {1018085, 1049922, 6533765}     703.0  \n",
              "32505                                          {6534178}     703.0  \n",
              "\n",
              "[32506 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ebecd830-ad1f-487a-8f42-f38993616b70\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>household_key</th>\n",
              "      <th>BASKET_ID</th>\n",
              "      <th>items</th>\n",
              "      <th>basket_day</th>\n",
              "      <th>next_items</th>\n",
              "      <th>next_day</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>35840912229</td>\n",
              "      <td>{7130924, 5577022, 995242, 1124029, 1088462, 8...</td>\n",
              "      <td>535</td>\n",
              "      <td>{819312}</td>\n",
              "      <td>536.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>35841522378</td>\n",
              "      <td>{819312}</td>\n",
              "      <td>536</td>\n",
              "      <td>{9527290}</td>\n",
              "      <td>537.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>35865806127</td>\n",
              "      <td>{9527290}</td>\n",
              "      <td>537</td>\n",
              "      <td>{1069297}</td>\n",
              "      <td>538.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>35877676640</td>\n",
              "      <td>{1069297}</td>\n",
              "      <td>538</td>\n",
              "      <td>{991580, 5569368, 5582712, 936685, 900875, 971...</td>\n",
              "      <td>540.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>36027807102</td>\n",
              "      <td>{991580, 5569368, 5582712, 936685, 900875, 971...</td>\n",
              "      <td>540</td>\n",
              "      <td>{5577022, 1069575, 856942, 961554}</td>\n",
              "      <td>546.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32501</th>\n",
              "      <td>998</td>\n",
              "      <td>41918011754</td>\n",
              "      <td>{1091365, 1134152, 1007136, 15830211}</td>\n",
              "      <td>683</td>\n",
              "      <td>{981760, 995242, 849843, 899624, 845208}</td>\n",
              "      <td>691.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32502</th>\n",
              "      <td>998</td>\n",
              "      <td>42031013564</td>\n",
              "      <td>{981760, 995242, 849843, 899624, 845208}</td>\n",
              "      <td>691</td>\n",
              "      <td>{962568}</td>\n",
              "      <td>693.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32503</th>\n",
              "      <td>998</td>\n",
              "      <td>42049756099</td>\n",
              "      <td>{962568}</td>\n",
              "      <td>693</td>\n",
              "      <td>{843338, 865874, 12524086}</td>\n",
              "      <td>694.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32504</th>\n",
              "      <td>998</td>\n",
              "      <td>42063285897</td>\n",
              "      <td>{843338, 865874, 12524086}</td>\n",
              "      <td>694</td>\n",
              "      <td>{1018085, 1049922, 6533765}</td>\n",
              "      <td>703.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32505</th>\n",
              "      <td>998</td>\n",
              "      <td>42185598715</td>\n",
              "      <td>{1018085, 1049922, 6533765}</td>\n",
              "      <td>703</td>\n",
              "      <td>{6534178}</td>\n",
              "      <td>703.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>32506 rows × 6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ebecd830-ad1f-487a-8f42-f38993616b70')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ebecd830-ad1f-487a-8f42-f38993616b70 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ebecd830-ad1f-487a-8f42-f38993616b70');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-006b9395-2834-412c-a801-a27897bab2d0\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-006b9395-2834-412c-a801-a27897bab2d0')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-006b9395-2834-412c-a801-a27897bab2d0 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_a467cd3f-f947-4134-8e16-7d2e1a0633f3\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('orders')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_a467cd3f-f947-4134-8e16-7d2e1a0633f3 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('orders');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "orders",
              "summary": "{\n  \"name\": \"orders\",\n  \"rows\": 32506,\n  \"fields\": [\n    {\n      \"column\": \"household_key\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 500,\n        \"samples\": [\n          \"400\",\n          \"1366\",\n          \"489\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"BASKET_ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 32506,\n        \"samples\": [\n          \"40680368840\",\n          \"41600145413\",\n          \"41466625862\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"items\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"basket_day\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 50,\n        \"min\": 532,\n        \"max\": 711,\n        \"num_unique_values\": 180,\n        \"samples\": [\n          649,\n          644,\n          618\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"next_items\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"next_day\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 50.80673968368635,\n        \"min\": 532.0,\n        \"max\": 711.0,\n        \"num_unique_values\": 180,\n        \"samples\": [\n          660.0,\n          644.0,\n          584.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transaction_data.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BT_E37EGov2y",
        "outputId": "e06caf8e-4318-4ff6-8c02-4894eda7e95a"
      },
      "id": "BT_E37EGov2y",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['household_key', 'BASKET_ID', 'DAY', 'PRODUCT_ID', 'QUANTITY',\n",
              "       'SALES_VALUE', 'STORE_ID', 'RETAIL_DISC', 'TRANS_TIME', 'WEEK_NO',\n",
              "       'COUPON_DISC', 'COUPON_MATCH_DISC', 'timestamp', 'hour', 'dayofweek',\n",
              "       'is_weekend', 'day', 'week', 'month', 'quarter', 'year', 'part_of_day',\n",
              "       'DEPARTMENT', 'MANUFACTURER', 'BRAND', 'COUPON_UPC', 'CAMPAIGN',\n",
              "       'PROMO_KEY'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3-5.Candidate Generation Baseline\n",
        "\n",
        "## Candidate Generation Overview\n",
        "We generate Top-K candidate promotions per query (household-day) using:\n",
        "- Active promo filtering by household and day.\n",
        "- Inverted index from `PRODUCT_ID → {PROMO_KEY}` to find promos overlapping user history.\n",
        "- Scoring by overlap + tiny popularity prior:\n",
        "  score(promo) = overlap(user_hist, promo_products) + 0.001 × global_popularity\n",
        "- Force-in positives: same-day redeemed promos are injected to the candidate list and deduped.\n",
        "- Fallback: fill remaining slots with active promos sorted by popularity.\n",
        "\n",
        "Goal:\n",
        "- High recall in retrieval so the ranker can reorder effectively at the final stage."
      ],
      "metadata": {
        "id": "TfFxDaR0W984"
      },
      "id": "TfFxDaR0W984"
    },
    {
      "cell_type": "code",
      "source": [
        "# check if that id got the promotion on specific day\n",
        "def get_active_promos_for_household_on_day(hh: str, day: int) -> list[str]:\n",
        "    rows = hh_promos[hh_promos['household_key'] == hh]\n",
        "    rows = rows[(rows['START_DAY'] <= int(day)) & (int(day) <= rows['END_DAY'])]\n",
        "    return rows['PROMO_KEY'].tolist()\n",
        "\n",
        "# create candidate list for hh-id in specific day\n",
        "def build_promo_candidates(household_key: str, user_hist_items: set[str], day: int, topk: int = TOPK_CANDS) -> list[str]:\n",
        "    active = get_active_promos_for_household_on_day(household_key, day) #check if it active for hh_id in that day\n",
        "    if not active:\n",
        "        return []\n",
        "    # score by overlap with user history + 0.001 x global popularity\n",
        "    def score(pkey: str) -> float:\n",
        "        prods = promo_to_products.get(pkey, set())\n",
        "        overlap = len(prods & set(user_hist_items or []))\n",
        "        pop = promo_pop.get(pkey, 0)\n",
        "        return overlap + 0.001 * pop\n",
        "    ranked = sorted(active, key=score, reverse=True)\n",
        "    return ranked[:topk]"
      ],
      "metadata": {
        "id": "QmMsExukCVTd"
      },
      "id": "QmMsExukCVTd",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3-6. Build (query, candidate) labeled rows\n",
        "\n",
        "#### Purpose\n",
        "Efficiently create `pair_df` of (query, candidate promo, label) using:\n",
        "- One query per household per day (`orders_day`).\n",
        "- `hist_exclusive` (history before the day) to avoid leakage.\n",
        "- Active promo filtering with LRU cache.\n",
        "- Inverted index from `PRODUCT_ID -> {PROMO_KEY}` to propose promos overlapping user history.\n",
        "- `heapq.nlargest` for Top-K by overlap + tiny popularity prior.\n",
        "- Fallback: fill remaining slots with most popular active promos.\n",
        "- Labels: same-day redemptions are forced into candidates and marked `label=1`.\n",
        "\n",
        "#### Outputs\n",
        "- `pair_df`: columns `household_key`, `BASKET_ID`, `PROMO_KEY`, `label`."
      ],
      "metadata": {
        "id": "z1Le-ykxXbvp"
      },
      "id": "z1Le-ykxXbvp"
    },
    {
      "cell_type": "code",
      "source": [
        "# Expect runtime : ~2 mins\n",
        "# 1 query per 1 household_id per day\n",
        "orders_day = (\n",
        "    orders.sort_values([\"household_key\",\"basket_day\",\"BASKET_ID\"])\n",
        "    .groupby([\"household_key\",\"basket_day\"], as_index=False)\n",
        "    .first()  # use only first basket_id on that day\n",
        "    .rename(columns={\"basket_day\":\"day_t\"})\n",
        ")\n",
        "# สร้าง hist_exclusive (ประวัติก่อนตะกร้าปัจจุบัน)\n",
        "def _add_hist_exclusive_per_day(grp):\n",
        "    hist = set()\n",
        "    res = []\n",
        "    for s in grp[\"items\"]:\n",
        "        res.append(hist.copy())\n",
        "        hist |= s\n",
        "    out = grp.copy()\n",
        "    out[\"hist_exclusive\"] = res\n",
        "    return out\n",
        "\n",
        "orders_fast = (\n",
        "    orders_day.sort_values([\"household_key\",\"day_t\"])\n",
        "    .groupby(\"household_key\", group_keys=False)\n",
        "    .apply(_add_hist_exclusive_per_day)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# 1) Pre-index promos per household (active filter เร็ว)\n",
        "hh_to_promos = {\n",
        "    hh: grp[[\"START_DAY\",\"END_DAY\",\"PROMO_KEY\"]].to_numpy()\n",
        "    for hh, grp in hh_promos.groupby(\"household_key\", sort=False)\n",
        "}\n",
        "\n",
        "@lru_cache(maxsize=1_000_000)\n",
        "def get_active_promos_tuple(hh: str, day: int) -> tuple:\n",
        "    arr = hh_to_promos.get(hh)\n",
        "    if arr is None:\n",
        "        return ()\n",
        "    d = int(day)\n",
        "    mask = (arr[:,0] <= d) & (d <= arr[:,1])\n",
        "    if not mask.any():\n",
        "        return ()\n",
        "    return tuple(arr[mask][:,2])\n",
        "\n",
        "# 2) Inverted index: PRODUCT_ID -> set(PROMO_KEY)\n",
        "product_to_promos = defaultdict(set)\n",
        "for pkey, prods in promo_to_products.items():\n",
        "    for pid in prods:\n",
        "        product_to_promos[pid].add(pkey)\n",
        "\n",
        "# 3) แคช active ที่เรียงตามความนิยมไว้เติม fallback\n",
        "@lru_cache(maxsize=1_000_000)\n",
        "def get_active_promos_sorted_by_pop(hh: str, day: int) -> tuple:\n",
        "    active = get_active_promos_tuple(hh, day)\n",
        "    if not active:\n",
        "        return ()\n",
        "    # เรียงตาม popularity จากมากไปน้อย\n",
        "    return tuple(sorted(active, key=lambda k: promo_pop.get(k, 0), reverse=True))\n",
        "\n",
        "# Locals\n",
        "empty_set = frozenset()\n",
        "promo_pop_local = promo_pop\n",
        "promo_to_products_local = promo_to_products\n",
        "redempt_by_hh_day_local = redempt_by_hh_day\n",
        "TOPK = TOPK_CANDS  # ปรับลงเป็น 20 ชั่วคราวได้ตอนเทสต์\n",
        "\n",
        "# 4) Build columns\n",
        "col_hh, col_bid, col_promo, col_label = [], [], [], []\n",
        "\n",
        "for r in orders_fast.itertuples(index=False):\n",
        "    hh = r.household_key\n",
        "    bid = r.BASKET_ID\n",
        "    day_t = int(r.day_t)\n",
        "    user_hist = r.hist_exclusive if isinstance(r.hist_exclusive, set) else set(r.hist_exclusive)\n",
        "\n",
        "    active = get_active_promos_tuple(hh, day_t)\n",
        "    if not active:\n",
        "        continue\n",
        "    active_set = set(active)\n",
        "\n",
        "    # 4.1) ใช้ inverted index สร้างชุด candidate จากสินค้าที่ user เคยซื้อ (ลดขนาดอย่างมาก)\n",
        "    cand_from_hist = set()\n",
        "    # จำกัดขนาดประวัติ (เช่น 200 ชิ้นล่าสุด) เพื่อลดเวลา\n",
        "    if len(user_hist) > 200:\n",
        "        # แปลงเป็น list แล้วหยิบต้นๆ พอเป็นตัวแทน (hist_exclusive เป็น set จึงไม่มีลำดับที่แท้)\n",
        "        user_hist_iter = list(user_hist)[:200]\n",
        "    else:\n",
        "        user_hist_iter = user_hist\n",
        "\n",
        "    for pid in user_hist_iter:\n",
        "        cand_from_hist |= product_to_promos.get(pid, empty_set)\n",
        "\n",
        "    # เฉพาะโปรโมชันที่ active จริงในวันนั้น\n",
        "    cand_from_hist &= active_set\n",
        "\n",
        "    # 4.2) ให้คะแนนเฉพาะชุดที่ overlap (เล็กลงมาก) แล้วคัด TopK\n",
        "    def score(pkey: str) -> float:\n",
        "        prods = promo_to_products_local.get(pkey, empty_set)\n",
        "        return (len(prods & user_hist) if prods else 0.0) + 0.001 * promo_pop_local.get(pkey, 0)\n",
        "\n",
        "    if cand_from_hist:\n",
        "        ranked_overlap = heapq.nlargest(TOPK, cand_from_hist, key=score)\n",
        "    else:\n",
        "        ranked_overlap = []\n",
        "\n",
        "    # 4.3) เติม fallback ด้วย active ที่นิยมสูง (ไม่ต้องคำนวณ score เพิ่ม)\n",
        "    if len(ranked_overlap) < TOPK:\n",
        "        need = TOPK - len(ranked_overlap)\n",
        "        fallback = []\n",
        "        seen = set(ranked_overlap)\n",
        "        for p in get_active_promos_sorted_by_pop(hh, day_t):\n",
        "            if p not in seen:\n",
        "                seen.add(p)\n",
        "                fallback.append(p)\n",
        "                if len(fallback) >= need:\n",
        "                    break\n",
        "        cands_base = ranked_overlap + fallback\n",
        "    else:\n",
        "        cands_base = ranked_overlap\n",
        "\n",
        "    # 4.4) ใส่ positives (same-day) ให้อยู่ต้นๆ แล้ว dedupe\n",
        "    positives = redempt_by_hh_day_local.get((hh, day_t), empty_set)\n",
        "    if positives:\n",
        "        pos_list = list(positives)\n",
        "        if pos_list:\n",
        "            seen = set()\n",
        "            cands = []\n",
        "            for p in pos_list:\n",
        "                if p not in seen:\n",
        "                    seen.add(p); cands.append(p)\n",
        "            for p in cands_base:\n",
        "                if p not in seen:\n",
        "                    seen.add(p); cands.append(p)\n",
        "            cands = cands[:TOPK]\n",
        "        else:\n",
        "            cands = cands_base[:TOPK]\n",
        "    else:\n",
        "        cands = cands_base[:TOPK]\n",
        "\n",
        "    if not cands:\n",
        "        continue\n",
        "\n",
        "    is_pos = positives.__contains__\n",
        "    col_hh.extend([hh] * len(cands))\n",
        "    col_bid.extend([int(bid)] * len(cands))\n",
        "    col_promo.extend(cands)\n",
        "    col_label.extend([1 if is_pos(p) else 0 for p in cands])\n",
        "\n",
        "pair_df = pd.DataFrame({\n",
        "    \"household_key\": col_hh,\n",
        "    \"BASKET_ID\": col_bid,\n",
        "    \"PROMO_KEY\": col_promo,\n",
        "    \"label\": col_label\n",
        "})\n",
        "pair_df.head(3)"
      ],
      "metadata": {
        "id": "ckiEfpD_Eg8T"
      },
      "id": "ckiEfpD_Eg8T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Downsample negatives (class balancing for training)\n",
        "\n",
        "#### Class Balancing via Negative Downsampling\n",
        "To stabilize training:\n",
        "- Keep all positives, sample at most `max_neg_per_pos` negatives per query.\n",
        "- Optionally drop groups with no positives.\n",
        "\n",
        "Outcome:\n",
        "- Reduces imbalance and training time.\n",
        "- Preserves the original validation distribution (no downsampling on valid)."
      ],
      "metadata": {
        "id": "FvOu26RAGNT9"
      },
      "id": "FvOu26RAGNT9"
    },
    {
      "cell_type": "code",
      "source": [
        "def downsample_negatives(df, max_neg_per_pos=30, drop_neg_only_groups=True):\n",
        "    def _sub(g):\n",
        "        pos = g[g.label == 1]\n",
        "        neg = g[g.label == 0]\n",
        "        if len(pos) == 0:\n",
        "            # ตัดกลุ่มนี้ทิ้งจาก train\n",
        "            return g.iloc[0:0] if drop_neg_only_groups else neg.sample(min(50, len(neg)), random_state=42)\n",
        "        keep_neg = neg.sample(min(len(pos) * max_neg_per_pos, len(neg)), random_state=42)\n",
        "        return pd.concat([pos, keep_neg], axis=0)\n",
        "    return df.groupby(['household_key','BASKET_ID'], group_keys=False).apply(_sub).reset_index(drop=True)\n",
        "\n",
        "pair_df = downsample_negatives(pair_df, max_neg_per_pos=30)"
      ],
      "metadata": {
        "id": "S25lfTtV1Rid"
      },
      "id": "S25lfTtV1Rid",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-7.PRomotion Feature Engineering\n",
        "\n",
        "#### Purpose\n",
        "Construct features per (query, promo):\n",
        "- `feat_promo_pop`: promo redemption popularity.\n",
        "- `feat_user_affinity`: sum of user’s purchases over products covered by the promo.\n",
        "- `promo_ohe` one-hot: aggregated `DEPARTMENT_*` and `BRAND_*`.\n",
        "\n",
        "Create `X` (features), `y` (labels), and `qid` (group id per query).\n",
        "\n",
        "#### Outputs\n",
        "- Feature matrix `X`, labels `y`, and grouping keys `qid`."
      ],
      "metadata": {
        "id": "-Uc5L4TYGWW7"
      },
      "id": "-Uc5L4TYGWW7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering (High-Level)\n",
        "We compose features at the (query, promo) level, grouped as:\n",
        "- Popularity (train-only)\n",
        "- Recency (train-only)\n",
        "- Affinity (train-only)\n",
        "- Coverage\n",
        "- Similarity (history overlap)\n",
        "- Temporal\n",
        "- Promo OHE (DEPARTMENT/BRAND)\n",
        "\n",
        "Key rule:\n",
        "- Any statistics must be computed using training data only, then applied to both train and validation rows."
      ],
      "metadata": {
        "id": "1Jd4L65d65KK"
      },
      "id": "1Jd4L65d65KK"
    },
    {
      "cell_type": "code",
      "source": [
        "# ตรวจ dtype key ให้ merge/group ได้เสมอ\n",
        "def normalize_keys(df, keys):\n",
        "    for k in keys:\n",
        "        df[k] = df[k].astype(str)\n",
        "    return df\n",
        "\n",
        "# สร้างฐาน (pair_df + บริบทจาก orders_fast)\n",
        "def build_base(pair_df, orders_fast):\n",
        "    normalize_keys(pair_df, ['household_key','BASKET_ID'])\n",
        "    normalize_keys(orders_fast, ['household_key','BASKET_ID'])\n",
        "\n",
        "    base = pair_df.merge(\n",
        "        orders_fast[['household_key','BASKET_ID','day_t','hist_exclusive']],\n",
        "        on=['household_key','BASKET_ID'], how='left'\n",
        "    )\n",
        "    # guard\n",
        "    assert {'household_key','BASKET_ID','PROMO_KEY','label','day_t','hist_exclusive'}.issubset(base.columns)\n",
        "    base = base.reset_index(drop=True).copy()\n",
        "    base['qid'] = base['household_key'].astype(str) + '_' + base['BASKET_ID'].astype(str)\n",
        "    base['label'] = base['label'].astype(int)\n",
        "    base['day_t'] = pd.to_numeric(base['day_t'], errors='coerce')\n",
        "    return base\n",
        "\n",
        "\n",
        "def make_scored_frame(model, Xv, base, valid_mask):\n",
        "    vf = base.loc[valid_mask, ['qid','label']].copy()\n",
        "    vf['score'] = model.predict(Xv)\n",
        "    # break ties to remove order bias from candidate construction\n",
        "    vf['score'] = vf['score'] + np.random.RandomState(42).normal(0, 1e-6, size=len(vf))\n",
        "    vf['rank'] = vf.groupby('qid')['score'].rank(ascending=False, method='first')\n",
        "    return vf\n",
        "\n",
        "def hitrate_at_k(df, k=10):\n",
        "    top = df.sort_values(['qid','score'], ascending=[True, False]).groupby('qid').head(k)\n",
        "    return (top.groupby('qid')['label'].max()).mean()\n",
        "\n",
        "def recall_precision_at_k(df, k=10):\n",
        "    got = df.sort_values(['qid','score'], ascending=[True, False]).groupby('qid').head(k)\n",
        "    pos_per_q = df.groupby('qid')['label'].sum()\n",
        "    hit_per_q = got.groupby('qid')['label'].sum()\n",
        "    recall = (hit_per_q / pos_per_q.replace(0, np.nan)).mean()\n",
        "    precision = got.groupby('qid')['label'].mean().mean()\n",
        "    return float(recall), float(precision)\n",
        "\n",
        "def mrr_at_k(df, k=10):\n",
        "    top = df.sort_values(['qid','score'], ascending=[True, False]).groupby('qid').head(k)\n",
        "    first_hit = top[top['label']==1].groupby('qid')['rank'].min()\n",
        "    return (1.0/first_hit).fillna(0).mean()\n",
        "\n",
        "def map_at_k(df, k=10):\n",
        "    tops = df.sort_values(['qid','score'], ascending=[True, False]).groupby('qid').head(k)\n",
        "    ap = []\n",
        "    for qid, sub in tops.groupby('qid'):\n",
        "        if sub['label'].sum() == 0:\n",
        "            continue\n",
        "        sub = sub.sort_values('rank')\n",
        "        cum_hits = sub['label'].cumsum()\n",
        "        prec_i = cum_hits / np.arange(1, len(sub)+1)\n",
        "        ap.append((prec_i * sub['label']).sum() / sub['label'].sum())\n",
        "    return np.mean(ap) if ap else 0.0"
      ],
      "metadata": {
        "id": "VWCy8okmJ4Qi"
      },
      "id": "VWCy8okmJ4Qi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Time-based Split & Purge Gap (Leakage-Free)\n",
        "We use a chronological split to avoid temporal leakage:\n",
        "- Sort by `day_t`, pick a cutoff by `test_ratio`, and apply a purge gap (e.g., 7 days).\n",
        "- Train uses days ≤ (cutoff − purge), Validation uses days ≥ cutoff.\n",
        "- We also assert there is no day overlap between train and valid.\n",
        "\n",
        "Why this matters:\n",
        "- Ensures all features (popularity, recency, affinity) can be computed from training history only.\n",
        "- Prevents “future info” from leaking into training.\n",
        "\n",
        "Example:\n",
        "- Train period: day 532–655\n",
        "- Purge gap: 7 days\n",
        "- Valid period: day 662–704"
      ],
      "metadata": {
        "id": "Tbs1LpqF6iEF"
      },
      "id": "Tbs1LpqF6iEF"
    },
    {
      "cell_type": "code",
      "source": [
        "def make_time_based_split(base, test_ratio=0.2, purge_days=7):\n",
        "\n",
        "    # เรียงเพื่อหา cutoff day (แต่ไม่เปลี่ยน index)\n",
        "    sorted_days = base['day_t'].sort_values().unique()\n",
        "\n",
        "    # หา cutoff day\n",
        "    cutoff_idx = int(len(sorted_days) * (1 - test_ratio))\n",
        "    cutoff_day = sorted_days[cutoff_idx]\n",
        "\n",
        "    # ใส่ purge gap\n",
        "    train_end_day = cutoff_day - purge_days\n",
        "    valid_start_day = cutoff_day\n",
        "\n",
        "    # สร้าง masks โดยใช้ index เดิมของ base\n",
        "    train_mask = base['day_t'] <= train_end_day\n",
        "    valid_mask = base['day_t'] >= valid_start_day\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"TIME-BASED SPLIT\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Train period: day {base.loc[train_mask, 'day_t'].min():.0f} - {base.loc[train_mask, 'day_t'].max():.0f}\")\n",
        "    print(f\"Purge gap:    {purge_days} days\")\n",
        "    print(f\"Valid period: day {base.loc[valid_mask, 'day_t'].min():.0f} - {base.loc[valid_mask, 'day_t'].max():.0f}\")\n",
        "    print(f\"\\nTrain samples: {train_mask.sum():,} ({train_mask.mean():.1%})\")\n",
        "    print(f\"Valid samples: {valid_mask.sum():,} ({valid_mask.mean():.1%})\")\n",
        "\n",
        "    # ตรวจสอบว่าไม่มี overlap\n",
        "    train_days_set = set(base.loc[train_mask, 'day_t'].unique())\n",
        "    valid_days_set = set(base.loc[valid_mask, 'day_t'].unique())\n",
        "    overlap = train_days_set & valid_days_set\n",
        "    assert len(overlap) == 0, f\"ERROR: Found {len(overlap)} overlapping days!\"\n",
        "    print(f\"\\n✅ No temporal overlap\")\n",
        "\n",
        "    # ตรวจสอบ (qid, PROMO_KEY) duplicates\n",
        "    train_pairs = set(base.loc[train_mask, ['qid','PROMO_KEY']].apply(tuple, axis=1))\n",
        "    valid_pairs = set(base.loc[valid_mask, ['qid','PROMO_KEY']].apply(tuple, axis=1))\n",
        "    dup_pairs = train_pairs & valid_pairs\n",
        "    print(f\"(qid, PROMO_KEY) duplicates: {len(dup_pairs)}\")\n",
        "    if len(dup_pairs) > 0:\n",
        "        print(f\"⚠️ WARNING: {len(dup_pairs)} duplicate pairs\")\n",
        "\n",
        "    # คำนวณ group sizes\n",
        "    grp_tr = base.loc[train_mask].groupby('qid').size().tolist()\n",
        "    grp_va = base.loc[valid_mask].groupby('qid').size().tolist()\n",
        "\n",
        "    print(f\"\\nTrain groups (queries): {len(grp_tr)}\")\n",
        "    print(f\"Valid groups (queries): {len(grp_va)}\")\n",
        "\n",
        "    return train_mask, valid_mask, grp_tr, grp_va\n",
        "\n",
        "# === Cutoff & Safety ===\n",
        "max_train_day = feat_base.loc[train_mask, 'day_t'].max()\n",
        "\n",
        "print(\"Train day range:\", feat_base.loc[train_mask, 'day_t'].min(), \"→\", max_train_day)\n",
        "print(\"Valid day range:\", feat_base.loc[valid_mask, 'day_t'].min(),  \"→\", feat_base.loc[valid_mask, 'day_t'].max())\n"
      ],
      "metadata": {
        "id": "4aBNwLMrjDjN"
      },
      "id": "4aBNwLMrjDjN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feat_base = build_base(pair_df, orders_fast)\n",
        "\n",
        "train_mask, valid_mask, grp_tr, grp_va = make_time_based_split(\n",
        "    feat_base,\n",
        "    test_ratio=0.2,\n",
        "    purge_days=7\n",
        ")\n",
        "\n",
        "y_tr = feat_base.loc[train_mask, 'label'].values\n",
        "y_va = feat_base.loc[valid_mask, 'label'].values\n",
        "\n",
        "# ⚠️ CRITICAL: เก็บ max_train_day สำหรับ feature engineering\n",
        "max_train_day = feat_base.loc[train_mask, 'day_t'].max()\n",
        "print(f\"\\n⚠️ Max train day for feature engineering: {max_train_day:.0f}\")\n",
        "assertexpected_max_train_day = feat_base.loc[train_mask, 'day_t'].max()\n",
        "print(f\"✅ Verified: max_train_day = {max_train_day:.0f} (matches train period)\")"
      ],
      "metadata": {
        "id": "hw2vQ1dtEHLx"
      },
      "id": "hw2vQ1dtEHLx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_eval_ranker(X_tr, y_tr, grp_tr, X_va, y_va, grp_va, base, valid_mask, eval_name=''):\n",
        "    assert not pd.isnull(X_tr).any().any()\n",
        "    assert not pd.isnull(X_va).any().any()\n",
        "\n",
        "    ranker = lgb.LGBMRanker(\n",
        "        objective='lambdarank', metric='ndcg',\n",
        "        boosting_type='gbdt',\n",
        "        num_leaves=127, learning_rate=0.05, n_estimators=1500,\n",
        "        subsample=0.8, colsample_bytree=0.8, random_state=RANDOM_STATE,early_stopping = 100\n",
        "    )\n",
        "    callbacks = [lgb.log_evaluation(100), lgb.early_stopping(50, first_metric_only=True)]\n",
        "    ranker.fit(\n",
        "        X_tr, y_tr, group=grp_tr,\n",
        "        eval_set=[(X_va, y_va)], eval_group=[grp_va],\n",
        "        eval_at=[5,10], callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    vf = make_scored_frame(ranker, X_va, base, valid_mask)\n",
        "    results = {}\n",
        "    for K in [1,3,5,10]:\n",
        "        r,p = recall_precision_at_k(vf, K)\n",
        "        results[f'Recall@{K}'] = r\n",
        "        results[f'Precision@{K}'] = p\n",
        "        results[f'HitRate@{K}'] = hitrate_at_k(vf, K)\n",
        "        results[f'MRR@{K}'] = mrr_at_k(vf, K)\n",
        "        results[f'MAP@{K}'] = map_at_k(vf, K)\n",
        "    return ranker, results"
      ],
      "metadata": {
        "id": "RMFgmkrAkGeo"
      },
      "id": "RMFgmkrAkGeo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transaction_data.columns"
      ],
      "metadata": {
        "id": "yCQ2k50FnvN6"
      },
      "id": "yCQ2k50FnvN6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ต้องมี coupon, product, transaction_data ในหน่วยความจำ\n",
        "assert {'COUPON_UPC','CAMPAIGN','PRODUCT_ID'}.issubset(coupon.columns)\n",
        "assert 'PRODUCT_ID' in product.columns\n",
        "\n",
        "# PROMO_KEY\n",
        "coupon = coupon.copy()\n",
        "coupon['PROMO_KEY'] = coupon['COUPON_UPC'].astype(str) + '|' + coupon['CAMPAIGN'].astype(str)\n",
        "\n",
        "# promo -> products\n",
        "promo_to_products = (\n",
        "    coupon[['PROMO_KEY','PRODUCT_ID']].drop_duplicates()\n",
        "    .groupby('PROMO_KEY')['PRODUCT_ID'].apply(lambda s: set(s.dropna()))\n",
        "    .to_dict()\n",
        ")\n",
        "\n",
        "\n",
        "promo_dept_set = (\n",
        "    coupon_prod[['PROMO_KEY','DEPARTMENT']].drop_duplicates()\n",
        "    .groupby('PROMO_KEY')['DEPARTMENT'].apply(lambda s: set(s.dropna()))\n",
        "    .to_dict()\n",
        ")\n",
        "\n",
        "promo_brand_set = (\n",
        "    coupon_prod[['PROMO_KEY','BRAND']].drop_duplicates()\n",
        "    .groupby('PROMO_KEY')['BRAND'].apply(lambda s: set(s.dropna()))\n",
        "    .to_dict()\n",
        ")"
      ],
      "metadata": {
        "id": "-VCwHWQHjBw5"
      },
      "id": "-VCwHWQHjBw5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recency Features (Train-Only)\n",
        "Compute last purchase day for each `(household, PRODUCT_ID)` using only training transactions, then map to promo products:\n",
        "- feat_min_recency = min(day_t − last_day)\n",
        "- feat_mean_recency = mean(day_t − last_day)\n",
        "- feat_seen_products = number of promo products seen in training history\n",
        "\n",
        "This captures how recently the user interacted with products covered by the promo."
      ],
      "metadata": {
        "id": "h83uMiyH7ajZ"
      },
      "id": "h83uMiyH7ajZ"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_recency_features(base, max_train_day):\n",
        "\n",
        "    print(f\"[Recency] Using transactions up to day {max_train_day:.0f} (train period)\")\n",
        "\n",
        "    # ⚠️ กรอง transaction_data ให้เหลือเฉพาะก่อน validation period\n",
        "    tx_train = transaction_data[transaction_data['DAY'] <= max_train_day].copy()\n",
        "\n",
        "    last_day = (\n",
        "        tx_train.groupby(['household_key','PRODUCT_ID'])['DAY']\n",
        "        .max().rename('last_day').reset_index()\n",
        "    )\n",
        "    promo_prod_map = (\n",
        "        coupon[['PROMO_KEY','PRODUCT_ID']].drop_duplicates()\n",
        "    )\n",
        "\n",
        "    tmp = (base[['household_key','PROMO_KEY','day_t']].drop_duplicates()\n",
        "           .merge(promo_prod_map, on='PROMO_KEY', how='left')\n",
        "           .merge(last_day, on=['household_key','PRODUCT_ID'], how='left'))\n",
        "    tmp = tmp[tmp['last_day'].notnull()].copy()\n",
        "    tmp = tmp[tmp['last_day'] <= tmp['day_t']]\n",
        "    tmp['recency'] = pd.to_numeric(tmp['day_t'], errors='coerce') - pd.to_numeric(tmp['last_day'], errors='coerce')\n",
        "\n",
        "    rec = tmp.groupby(['household_key','PROMO_KEY']).agg(\n",
        "        feat_min_recency=('recency','min'),\n",
        "        feat_mean_recency=('recency','mean'),\n",
        "        feat_seen_products=('PRODUCT_ID','nunique')\n",
        "    ).reset_index()\n",
        "\n",
        "    out = base[['household_key','PROMO_KEY']].copy()\n",
        "    out = out.merge(rec, on=['household_key','PROMO_KEY'], how='left')\n",
        "    out[['feat_min_recency','feat_mean_recency']] = out[['feat_min_recency','feat_mean_recency']].fillna(9999)\n",
        "    out['feat_seen_products'] = out['feat_seen_products'].fillna(0).astype(int)\n",
        "    return out[['feat_min_recency','feat_mean_recency','feat_seen_products']].set_index(base.index)\n",
        "\n",
        "print(\"✅ Fixed build_recency_features\")\n",
        "\n",
        "# ⚠️ เปลี่ยนจาก train_mask เป็น max_train_day\n",
        "feat_recency = build_recency_features(feat_base, max_train_day)"
      ],
      "metadata": {
        "id": "zlLsufLNjG-U"
      },
      "id": "zlLsufLNjG-U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Affinity Features (Train-Only)\n",
        "Aggregate training transactions by `(household, DEPARTMENT)` and `(household, BRAND)`:\n",
        "- feat_user_dept_aff: sum of user’s counts over promo’s departments\n",
        "- feat_user_brand_aff: sum of user’s counts over promo’s brands\n",
        "\n",
        "This measures the user’s affinity to the promo’s brand/department footprint."
      ],
      "metadata": {
        "id": "ODqR1TAw7kco"
      },
      "id": "ODqR1TAw7kco"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_user_affinity_features(base, max_train_day):\n",
        "    \"\"\"\n",
        "    ⚠️ FIXED: คำนวณ user affinity จาก TRAIN data only เพื่อหลีกเลี่ยง temporal leakage\n",
        "    รับ max_train_day โดยตรงแทน train_mask\n",
        "    \"\"\"\n",
        "    print(f\"[Affinity] Using transactions up to day {max_train_day:.0f} (train period)\")\n",
        "\n",
        "    # ⚠️ กรอง transaction_data ให้เหลือเฉพาะก่อน validation period\n",
        "    tx = transaction_data[transaction_data['DAY'] <= max_train_day][['household_key','DEPARTMENT','BRAND']].copy()\n",
        "\n",
        "    tx['household_key'] = tx['household_key'].astype(str)\n",
        "    tx['DEPARTMENT'] = tx['DEPARTMENT'].fillna('UNK')\n",
        "    tx['BRAND'] = tx['BRAND'].fillna('UNK')\n",
        "\n",
        "    user_dept_ct = (\n",
        "        tx.groupby(['household_key','DEPARTMENT']).size()\n",
        "        .rename('user_dept_ct')\n",
        "    )\n",
        "    user_brand_ct = (\n",
        "        tx.groupby(['household_key','BRAND']).size()\n",
        "        .rename('user_brand_ct')\n",
        "    )\n",
        "\n",
        "    def sum_dept(row):\n",
        "        depts = promo_dept_set.get(row['PROMO_KEY'], set())\n",
        "        return int(sum(user_dept_ct.get((row['household_key'], d), 0) for d in depts))\n",
        "    def sum_brand(row):\n",
        "        brands = promo_brand_set.get(row['PROMO_KEY'], set())\n",
        "        return int(sum(user_brand_ct.get((row['household_key'], b), 0) for b in brands))\n",
        "\n",
        "    out = base[['household_key','PROMO_KEY']].copy()\n",
        "    out['feat_user_dept_aff'] = out.apply(sum_dept, axis=1)\n",
        "    out['feat_user_brand_aff'] = out.apply(sum_brand, axis=1)\n",
        "    return out[['feat_user_dept_aff','feat_user_brand_aff']].set_index(base.index)\n",
        "\n",
        "print(\"✅ Fixed build_user_affinity_features\")\n",
        "\n",
        "# ⚠️ เปลี่ยนจาก train_mask เป็น max_train_day\n",
        "feat_affinity = build_user_affinity_features(feat_base, max_train_day)"
      ],
      "metadata": {
        "id": "g41zk8QfjIns"
      },
      "id": "g41zk8QfjIns",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Coverage Features\n",
        "Static descriptors of the promo:\n",
        "- feat_num_promo_products\n",
        "- feat_num_promo_departments\n",
        "- feat_num_promo_brands\n",
        "\n",
        "These capture breadth/variety of items the promo covers."
      ],
      "metadata": {
        "id": "LnOtuBKs7z7K"
      },
      "id": "LnOtuBKs7z7K"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_coverage_features(base):\n",
        "    out = base[['PROMO_KEY']].copy()\n",
        "    out['feat_num_promo_products'] = out['PROMO_KEY'].map(lambda k: len(promo_to_products.get(k, set()))).astype(int)\n",
        "    out['feat_num_promo_departments'] = out['PROMO_KEY'].map(lambda k: len(promo_dept_set.get(k, set()))).astype(int)\n",
        "    out['feat_num_promo_brands'] = out['PROMO_KEY'].map(lambda k: len(promo_brand_set.get(k, set()))).astype(int)\n",
        "    return out[['feat_num_promo_products','feat_num_promo_departments','feat_num_promo_brands']].set_index(base.index)\n",
        "\n",
        "feat_coverage = build_coverage_features(feat_base)"
      ],
      "metadata": {
        "id": "KmZmBxrhjKBU"
      },
      "id": "KmZmBxrhjKBU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_promo_ohe_features(base):\n",
        "    ohe = promo_ohe.copy()\n",
        "    if ohe.index.name != 'PROMO_KEY':\n",
        "        if 'PROMO_KEY' in ohe.columns:\n",
        "            ohe = ohe.set_index('PROMO_KEY')\n",
        "    out = ohe.reindex(base['PROMO_KEY']).fillna(0)\n",
        "    for c in out.columns:\n",
        "        if out[c].dtype == bool:\n",
        "            out[c] = out[c].astype(np.uint8)\n",
        "        elif out[c].dtype == object:\n",
        "            out[c] = pd.to_numeric(out[c], errors='coerce').fillna(0).astype(np.uint8)\n",
        "    out.index = base.index\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "wozOKO1Ik1K7"
      },
      "id": "wozOKO1Ik1K7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Popularity Features (Train-Only)\n",
        "For each `PROMO_KEY` in train:\n",
        "- promo_offer_ct: number of offers shown\n",
        "- promo_pos_ct: number of redemptions\n",
        "- promo_unique_users: unique households exposed\n",
        "- feat_promo_pos_rate = promo_pos_ct / promo_offer_ct\n",
        "\n",
        "Note: Values for unseen promos in validation are safely set to zeros (no leakage)."
      ],
      "metadata": {
        "id": "qOlUufxl7Lh7"
      },
      "id": "qOlUufxl7Lh7"
    },
    {
      "cell_type": "code",
      "source": [
        "# === Train-only popularity (global) ===\n",
        "train_pairs = pair_df.loc[train_mask]  # IMPORTANT: อิง train_mask จาก time-split เท่านั้น\n",
        "promo_stats_tr = (train_pairs.groupby('PROMO_KEY')['label']\n",
        "                  .agg(offer_ct='count', pos_ct='sum')\n",
        "                  .assign(pos_rate=lambda x: x['pos_ct'] / x['offer_ct'].clip(lower=1)))\n",
        "feat_base = feat_base.join(promo_stats_tr, on='PROMO_KEY')\n",
        "feat_base[['offer_ct','pos_ct','pos_rate']] = feat_base[['offer_ct','pos_ct','pos_rate']].fillna(0)\n"
      ],
      "metadata": {
        "id": "mWiVRP5AyUHO"
      },
      "id": "mWiVRP5AyUHO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_popularity_features(base, train_mask=None):\n",
        "    \"\"\"\n",
        "    คำนวณ popularity features จาก TRAIN ONLY\n",
        "    แล้ว apply ไปทั้ง train และ valid\n",
        "    \"\"\"\n",
        "    # คำนวณจาก train เท่านั้น\n",
        "    train_data = base.loc[train_mask]\n",
        "\n",
        "    agg = train_data.groupby('PROMO_KEY').agg(\n",
        "        promo_offer_ct=('label', 'size'),\n",
        "        promo_pos_ct=('label', 'sum'),\n",
        "        promo_unique_users=('household_key', 'nunique')\n",
        "    )\n",
        "\n",
        "    # Positive rate (redemption rate)\n",
        "    agg['feat_promo_pos_rate'] = (agg['promo_pos_ct'] / agg['promo_offer_ct']).fillna(0.0)\n",
        "\n",
        "    agg = agg[['promo_offer_ct', 'promo_pos_ct', 'promo_unique_users', 'feat_promo_pos_rate']].reset_index()\n",
        "\n",
        "    # Join กลับไปทั้ง base (ทั้ง train และ valid)\n",
        "    out = base[['PROMO_KEY']].merge(agg, on='PROMO_KEY', how='left')\n",
        "\n",
        "    # Fill missing (promos ที่ไม่เคยปรากฏใน train)\n",
        "    out = out.fillna({\n",
        "        'promo_offer_ct': 0,\n",
        "        'promo_pos_ct': 0,\n",
        "        'promo_unique_users': 0,\n",
        "        'feat_promo_pos_rate': 0.0\n",
        "    })\n",
        "\n",
        "    # เช็คว่า valid มี new promos เท่าไร\n",
        "    valid_new_promos = (out.loc[~train_mask, 'promo_offer_ct'] == 0).sum()\n",
        "    if valid_new_promos > 0:\n",
        "        print(f\"⚠️ WARNING: {valid_new_promos} validation samples have unseen promos (will use default values)\")\n",
        "\n",
        "    return out[['promo_offer_ct', 'promo_pos_ct', 'promo_unique_users', 'feat_promo_pos_rate']].set_index(base.index)\n",
        "\n",
        "\n",
        "# ใช้งาน: แทนที่ Cell 56\n",
        "# ⚠️ ต้อง build หลังจากมี train_mask จาก time-based split แล้ว\n",
        "feat_popularity = build_popularity_features(feat_base, train_mask)\n"
      ],
      "metadata": {
        "id": "9HP1cHMtjLSE"
      },
      "id": "9HP1cHMtjLSE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Build cumulative history per (household_key, day_t) ===\n",
        "def build_hist_until_day(transaction_data):\n",
        "    tx = transaction_data.sort_values(['household_key','DAY','BASKET_ID'])\n",
        "    out = []\n",
        "    seen = {}  # hh -> set(PRODUCT_ID)\n",
        "    for (hh, day), g in tx.groupby(['household_key','DAY'], sort=False):\n",
        "        hist = seen.get(hh, set()).copy()  # อดีตล้วน ก่อน day_t\n",
        "        out.append({'household_key': hh, 'day_t': int(day), 'hist_until_t': hist})\n",
        "        items_today = set(g['PRODUCT_ID'].astype(str).unique())\n",
        "        seen.setdefault(hh, set()).update(items_today)  # อัปเดตหลังบันทึก\n",
        "    return (pd.DataFrame(out)\n",
        "            .sort_values(['household_key','day_t'])\n",
        "            .reset_index(drop=True))\n",
        "\n",
        "hist_df = build_hist_until_day(transaction_data)\n",
        "feat_base = feat_base.merge(hist_df, on=['household_key','day_t'], how='left')\n",
        "feat_base['hist_until_t'] = feat_base['hist_until_t'].apply(lambda s: s if isinstance(s,set) else set())\n"
      ],
      "metadata": {
        "id": "Yf7UnEPayIrF"
      },
      "id": "Yf7UnEPayIrF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Similarity Features (History Overlap)\n",
        "Using `hist_until_t` (history strictly before day_t):\n",
        "- feat_hist_size = |history|\n",
        "- feat_overlap_cnt = |promo_products ∩ history|\n",
        "- feat_jaccard = |∩| / |∪|\n",
        "- feat_overlap_ratio = |∩| / |history|\n",
        "\n",
        "These measure how aligned the user’s past purchases are with the promo’s coverage."
      ],
      "metadata": {
        "id": "RwBrSL1974FJ"
      },
      "id": "RwBrSL1974FJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# === Similarity features (use hist_until_t) ===\n",
        "import numpy as np\n",
        "sim = pd.DataFrame(index=feat_base.index)\n",
        "\n",
        "def promo_set(promo_key):\n",
        "    return promo_to_products.get(promo_key, set())\n",
        "\n",
        "hist_sizes = feat_base['hist_until_t'].apply(len)\n",
        "overlaps   = (feat_base\n",
        "              .apply(lambda r: len(promo_set(r['PROMO_KEY']) & r['hist_until_t']), axis=1))\n",
        "union_sz   = (feat_base\n",
        "              .apply(lambda r: len(promo_set(r['PROMO_KEY']) | r['hist_until_t']), axis=1))\n",
        "\n",
        "sim['feat_hist_size']     = hist_sizes.astype('int32')\n",
        "sim['feat_overlap_cnt']   = overlaps.astype('int16')\n",
        "sim['feat_jaccard']       = (overlaps / union_sz.replace(0, np.nan)).fillna(0.0).astype('float32')\n",
        "sim['feat_overlap_ratio'] = (overlaps / hist_sizes.replace(0, np.nan)).fillna(0.0).astype('float32')\n",
        "\n",
        "feat_similarity = sim\n"
      ],
      "metadata": {
        "id": "uXegp1ulpjbX"
      },
      "id": "uXegp1ulpjbX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Temporal Features\n",
        "- feat_dayofweek = day_t % 7\n",
        "- feat_is_weekend = 1 if dayofweek ∈ {5,6} else 0\n",
        "\n",
        "Temporal signals help capture weekly shopping patterns without using future info."
      ],
      "metadata": {
        "id": "8Pl7kFFj754z"
      },
      "id": "8Pl7kFFj754z"
    },
    {
      "cell_type": "code",
      "source": [
        "def build_temporal_features(base):\n",
        "    out = base[['day_t']].copy()\n",
        "    day_t = pd.to_numeric(out['day_t'], errors='coerce').fillna(0).astype(int)\n",
        "    out['feat_dayofweek'] = (day_t % 7).astype(np.int8)\n",
        "    out['feat_is_weekend'] = out['feat_dayofweek'].isin([5,6]).astype(np.int8)\n",
        "    return out[['feat_dayofweek','feat_is_weekend']].set_index(base.index)\n",
        "\n",
        "feat_temporal = build_temporal_features(feat_base)"
      ],
      "metadata": {
        "id": "gN5Is0a2jM8B"
      },
      "id": "gN5Is0a2jM8B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## From Feature Groups to Training Matrices\n",
        "We evaluate each feature group individually and via forward selection:\n",
        "- Train a ranker per group to measure standalone utility.\n",
        "- Iteratively add the next-best group and re-train.\n",
        "- Select the cumulative set that maximizes validation metrics (e.g., MRR@10, Recall@10).\n",
        "\n",
        "Example (current run):\n",
        "Selected groups = [popularity, recency, temporal, coverage, affinity, promo_ohe]\n",
        "Total features = 41"
      ],
      "metadata": {
        "id": "7ia9Xjk-79TX"
      },
      "id": "7ia9Xjk-79TX"
    },
    {
      "cell_type": "code",
      "source": [
        "feat_promo_ohe = build_promo_ohe_features(feat_base)\n",
        "print(\"✓ feat_promo_ohe\")\n",
        "feature_groups = {\n",
        "    'similarity': feat_similarity,\n",
        "    'recency':    feat_recency,\n",
        "    'affinity':   feat_affinity,\n",
        "    'coverage':   feat_coverage,\n",
        "    'popularity': feat_popularity,\n",
        "    'temporal':   feat_temporal,\n",
        "    'promo_ohe':  feat_promo_ohe,\n",
        "}\n",
        "# sanity\n",
        "for name, df in feature_groups.items():\n",
        "    assert (df.index == feat_base.index).all(), f'Index mismatch in {name}'"
      ],
      "metadata": {
        "id": "OpihBoXRjOTN"
      },
      "id": "OpihBoXRjOTN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sanity Check: Random Label Test\n",
        "Procedure:\n",
        "- Shuffle training labels, train the ranker, then evaluate on real validation labels.\n",
        "Expected:\n",
        "- NDCG@10 ≈ 0.10–0.25 (near random)\n",
        "Interpretation:\n",
        "- Higher values suggest memorization or leakage. We mitigate order/tie bias by adding a tiny jitter to prediction scores before ranking."
      ],
      "metadata": {
        "id": "4HucTqai8i_G"
      },
      "id": "4HucTqai8i_G"
    },
    {
      "cell_type": "code",
      "source": [
        "def sanity_check_random_labels(X_tr, y_tr, grp_tr, X_va, y_va, grp_va):\n",
        "    \"\"\"\n",
        "    Train โมเดลด้วย random labels แล้ววัด NDCG บน valid (real labels)\n",
        "    Expected: NDCG@10 ควร ~0.1-0.2 (baseline random)\n",
        "    \"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"SANITY CHECK: Random Label Test\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Shuffle labels\n",
        "    np.random.seed(999)\n",
        "    y_tr_random = np.random.permutation(y_tr)\n",
        "\n",
        "    # Train\n",
        "    ranker_random = lgb.LGBMRanker(\n",
        "        objective='lambdarank', metric='ndcg',\n",
        "        boosting_type='gbdt', num_leaves=31,\n",
        "        learning_rate=0.05, n_estimators=100,\n",
        "        subsample=0.8, random_state=42, verbose=-1\n",
        "    )\n",
        "\n",
        "    ranker_random.fit(\n",
        "        X_tr, y_tr_random, group=grp_tr,\n",
        "        eval_set=[(X_va, y_va)], eval_group=[grp_va],\n",
        "        eval_at=[10],\n",
        "        callbacks=[lgb.early_stopping(20, verbose=False)]\n",
        "    )\n",
        "\n",
        "    # Predict\n",
        "    scores = ranker_random.predict(X_va)\n",
        "    # break ties so group ordering doesn't boost NDCG spuriously\n",
        "    scores = scores + np.random.RandomState(999).normal(0, 1e-6, size=len(scores))\n",
        "\n",
        "    # คำนวณ NDCG@10 per group\n",
        "    ndcg_scores = []\n",
        "    start = 0\n",
        "    for grp in grp_va:\n",
        "        end = start + grp\n",
        "        y_true = y_va[start:end]\n",
        "        y_pred = scores[start:end]\n",
        "\n",
        "        if y_true.sum() > 0:  # มี positive\n",
        "            # ndcg_score ต้องการ shape (1, n)\n",
        "            ndcg = ndcg_score(y_true.reshape(1, -1), y_pred.reshape(1, -1), k=10)\n",
        "            ndcg_scores.append(ndcg)\n",
        "\n",
        "        start = end\n",
        "\n",
        "    avg_ndcg = np.mean(ndcg_scores) if ndcg_scores else 0.0\n",
        "\n",
        "    print(f\"\\nNDCG@10 with RANDOM labels: {avg_ndcg:.4f}\")\n",
        "    print(f\"Expected (baseline):        0.10 - 0.25\")\n",
        "\n",
        "    if avg_ndcg < 0.30:\n",
        "        print(\"✅ PASS: Random NDCG is low (no memorization)\")\n",
        "    else:\n",
        "        print(\"❌ FAIL: Random NDCG is high (possible overfitting/leakage)\")\n",
        "\n",
        "    return avg_ndcg\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lRkcJZt66V7c"
      },
      "id": "lRkcJZt66V7c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sanity Check: Hold-out Day-of-Week\n",
        "Procedure:\n",
        "- Train after excluding one day-of-week (e.g., Saturday), evaluate only on that day.\n",
        "Interpretation:\n",
        "- If NDCG@10 remains reasonable (> 0.25), the model is not overfitting to day-of-week artifacts."
      ],
      "metadata": {
        "id": "2ekpHLTO8mYE"
      },
      "id": "2ekpHLTO8mYE"
    },
    {
      "cell_type": "code",
      "source": [
        "def sanity_check_holdout_dow(feat_base, X_all, y_all, grp_all, dow_to_holdout=5):\n",
        "    \"\"\"\n",
        "    Train โดยตัด day-of-week ใดวันหนึ่งออก แล้วทดสอบบนวันนั้นเท่านั้น\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    dow_to_holdout : int\n",
        "        วันที่จะ hold out (0=Mon, 5=Sat, 6=Sun)\n",
        "    \"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(f\"SANITY CHECK: Hold-out Day {dow_to_holdout} (0=Mon, 5=Sat)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # สร้าง dow column ถ้ายังไม่มี\n",
        "    if 'feat_dayofweek' not in feat_base.columns:\n",
        "        feat_base['feat_dayofweek'] = feat_base['day_t'].astype(int) % 7\n",
        "\n",
        "    # Split\n",
        "    train_mask_no_dow = feat_base['feat_dayofweek'] != dow_to_holdout\n",
        "    valid_mask_dow_only = feat_base['feat_dayofweek'] == dow_to_holdout\n",
        "\n",
        "    print(f\"\\nTrain (without day {dow_to_holdout}): {train_mask_no_dow.sum():,} samples\")\n",
        "    print(f\"Valid (only day {dow_to_holdout}):    {valid_mask_dow_only.sum():,} samples\")\n",
        "\n",
        "    if valid_mask_dow_only.sum() == 0:\n",
        "        print(\"⚠️ SKIP: No samples on this day\")\n",
        "        return None\n",
        "\n",
        "    # Subset\n",
        "    X_tr_no_dow = X_all[train_mask_no_dow]\n",
        "    y_tr_no_dow = y_all[train_mask_no_dow]\n",
        "    grp_tr_no_dow = feat_base.loc[train_mask_no_dow].groupby('qid').size().tolist()\n",
        "\n",
        "    X_va_dow = X_all[valid_mask_dow_only]\n",
        "    y_va_dow = y_all[valid_mask_dow_only]\n",
        "    grp_va_dow = feat_base.loc[valid_mask_dow_only].groupby('qid').size().tolist()\n",
        "\n",
        "    # Train\n",
        "    ranker_no_dow = lgb.LGBMRanker(\n",
        "        objective='lambdarank', metric='ndcg',\n",
        "        boosting_type='gbdt', num_leaves=63,\n",
        "        learning_rate=0.05, n_estimators=300,\n",
        "        subsample=0.8, random_state=42, verbose=-1\n",
        "    )\n",
        "\n",
        "    ranker_no_dow.fit(\n",
        "        X_tr_no_dow, y_tr_no_dow, group=grp_tr_no_dow,\n",
        "        eval_set=[(X_va_dow, y_va_dow)], eval_group=[grp_va_dow],\n",
        "        eval_at=[10],\n",
        "        callbacks=[lgb.early_stopping(30, verbose=False)]\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    scores = ranker_no_dow.predict(X_va_dow)\n",
        "\n",
        "    ndcg_scores = []\n",
        "    start = 0\n",
        "    for grp in grp_va_dow:\n",
        "        end = start + grp\n",
        "        y_true = y_va_dow[start:end]\n",
        "        y_pred = scores[start:end]\n",
        "\n",
        "        if y_true.sum() > 0:\n",
        "            ndcg = ndcg_score(y_true.reshape(1, -1), y_pred.reshape(1, -1), k=10)\n",
        "            ndcg_scores.append(ndcg)\n",
        "\n",
        "        start = end\n",
        "\n",
        "    avg_ndcg = np.mean(ndcg_scores) if ndcg_scores else 0.0\n",
        "\n",
        "    print(f\"\\nNDCG@10 on held-out day {dow_to_holdout}: {avg_ndcg:.4f}\")\n",
        "\n",
        "    # เปรียบเทียบกับ baseline\n",
        "    if avg_ndcg > 0.3:\n",
        "        print(\"✅ GOOD: Model generalizes well without this day\")\n",
        "    elif avg_ndcg > 0.15:\n",
        "        print(\"⚠️ WARNING: Model struggles without this day (moderate DOW dependency)\")\n",
        "    else:\n",
        "        print(\"❌ FAIL: Model fails without this day (strong temporal leakage)\")\n",
        "\n",
        "    return avg_ndcg\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FOmhz4qfBmAz"
      },
      "id": "FOmhz4qfBmAz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_rows = []\n",
        "for name, F in feature_groups.items():\n",
        "    X_tr = F.loc[train_mask].copy()\n",
        "    X_va = F.loc[valid_mask].copy()\n",
        "\n",
        "    # แปลง categorical (ถ้าเจอ) ให้เป็นตัวเลข/one-hot (ในชุดนี้เป็นตัวเลขอยู่แล้ว)\n",
        "    for c in X_tr.columns:\n",
        "        if X_tr[c].dtype == 'bool':\n",
        "            X_tr[c] = X_tr[c].astype(np.uint8)\n",
        "            X_va[c] = X_va[c].astype(np.uint8)\n",
        "\n",
        "    _, res = train_and_eval_ranker(\n",
        "        X_tr, y_tr, grp_tr,\n",
        "        X_va, y_va, grp_va,\n",
        "        base=feat_base, valid_mask=valid_mask,\n",
        "        eval_name=name\n",
        "    )\n",
        "    res_row = {'group': name}\n",
        "    res_row.update(res)\n",
        "    results_rows.append(res_row)\n",
        "\n",
        "single_group_results = pd.DataFrame(results_rows).sort_values('MRR@10', ascending=False)\n",
        "single_group_results"
      ],
      "metadata": {
        "id": "gqWsx9gKjRVW"
      },
      "id": "gqWsx9gKjRVW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_comprehensive_stats(feat_base, train_mask, valid_mask):\n",
        "    \"\"\"\n",
        "    แสดงสถิติครอบคลุมสำหรับรายงาน\n",
        "    \"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"COMPREHENSIVE STATISTICS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 1. Group sizes\n",
        "    print(\"\\n[1] GROUP SIZE STATISTICS\")\n",
        "    grp_size_train = feat_base.loc[train_mask].groupby('qid').size()\n",
        "    grp_size_valid = feat_base.loc[valid_mask].groupby('qid').size()\n",
        "\n",
        "    print(\"\\nTrain:\")\n",
        "    print(grp_size_train.describe())\n",
        "\n",
        "    print(\"\\nValid:\")\n",
        "    print(grp_size_valid.describe())\n",
        "\n",
        "    # 2. Label distribution\n",
        "    print(\"\\n[2] LABEL DISTRIBUTION\")\n",
        "    train_pos = feat_base.loc[train_mask, 'label'].sum()\n",
        "    train_total = train_mask.sum()\n",
        "    valid_pos = feat_base.loc[valid_mask, 'label'].sum()\n",
        "    valid_total = valid_mask.sum()\n",
        "\n",
        "    print(f\"Train: {train_pos:,} positives / {train_total:,} total ({train_pos/train_total:.2%})\")\n",
        "    print(f\"Valid: {valid_pos:,} positives / {valid_total:,} total ({valid_pos/valid_total:.2%})\")\n",
        "\n",
        "    # 3. Groups without negatives\n",
        "    train_all_pos = (feat_base.loc[train_mask].groupby('qid')['label'].sum() ==\n",
        "                     feat_base.loc[train_mask].groupby('qid')['label'].count()).sum()\n",
        "    valid_all_pos = (feat_base.loc[valid_mask].groupby('qid')['label'].sum() ==\n",
        "                     feat_base.loc[valid_mask].groupby('qid')['label'].count()).sum()\n",
        "\n",
        "    print(f\"\\nGroups without negatives:\")\n",
        "    print(f\"Train: {train_all_pos} / {len(grp_size_train)} ({train_all_pos/len(grp_size_train):.2%})\")\n",
        "    print(f\"Valid: {valid_all_pos} / {len(grp_size_valid)} ({valid_all_pos/len(grp_size_valid):.2%})\")\n",
        "\n",
        "    # 4. Positive rate by DOW\n",
        "    if 'feat_dayofweek' in feat_base.columns:\n",
        "        print(\"\\n[3] POSITIVE RATE BY DAY-OF-WEEK\")\n",
        "        print(\"\\nTrain:\")\n",
        "        print(feat_base.loc[train_mask].groupby('feat_dayofweek')['label'].mean().to_string())\n",
        "        print(\"\\nValid:\")\n",
        "        print(feat_base.loc[valid_mask].groupby('feat_dayofweek')['label'].mean().to_string())\n",
        "\n",
        "    # 5. Temporal coverage\n",
        "    print(\"\\n[4] TEMPORAL COVERAGE\")\n",
        "    print(f\"Train days: {feat_base.loc[train_mask, 'day_t'].min():.0f} - {feat_base.loc[train_mask, 'day_t'].max():.0f} \"\n",
        "          f\"({feat_base.loc[train_mask, 'day_t'].nunique()} unique days)\")\n",
        "    print(f\"Valid days: {feat_base.loc[valid_mask, 'day_t'].min():.0f} - {feat_base.loc[valid_mask, 'day_t'].max():.0f} \"\n",
        "          f\"({feat_base.loc[valid_mask, 'day_t'].nunique()} unique days)\")\n",
        "\n",
        "\n",
        "# ใช้งาน\n",
        "display_comprehensive_stats(feat_base, train_mask, valid_mask)\n",
        "\n"
      ],
      "metadata": {
        "id": "f6XuUpAqBuKA"
      },
      "id": "f6XuUpAqBuKA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "order = single_group_results['group'].tolist()  # เรียงจาก MRR@10 ดีสุดก่อน\n",
        "cum_results = []\n",
        "X_tr_cum = None\n",
        "X_va_cum = None\n",
        "\n",
        "for i, g in enumerate(order, 1):\n",
        "    F = feature_groups[g]\n",
        "    if X_tr_cum is None:\n",
        "        X_tr_cum = F.loc[train_mask].copy()\n",
        "        X_va_cum = F.loc[valid_mask].copy()\n",
        "    else:\n",
        "        X_tr_cum = pd.concat([X_tr_cum, F.loc[train_mask].copy()], axis=1)\n",
        "        X_va_cum = pd.concat([X_va_cum, F.loc[valid_mask].copy()], axis=1)\n",
        "\n",
        "    # cast bool → uint8\n",
        "    for c in X_tr_cum.columns:\n",
        "        if X_tr_cum[c].dtype == 'bool':\n",
        "            X_tr_cum[c] = X_tr_cum[c].astype(np.uint8)\n",
        "            X_va_cum[c] = X_va_cum[c].astype(np.uint8)\n",
        "\n",
        "    _, res = train_and_eval_ranker(\n",
        "        X_tr_cum, y_tr, grp_tr,\n",
        "        X_va_cum, y_va, grp_va,\n",
        "        base=feat_base, valid_mask=valid_mask,\n",
        "        eval_name=f'forward_{i}_{g}'\n",
        "    )\n",
        "    res_row = {'step': i, 'added_group': g}\n",
        "    res_row.update(res)\n",
        "    cum_results.append(res_row)\n",
        "\n",
        "forward_selection_results = pd.DataFrame(cum_results).sort_values('step')\n",
        "forward_selection_results"
      ],
      "metadata": {
        "id": "BflfCeoFjTYt"
      },
      "id": "BflfCeoFjTYt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_step = forward_selection_results.sort_values('MRR@10', ascending=False).iloc[0]['step']\n",
        "selected_groups = single_group_results['group'].tolist()[:int(best_step)]\n",
        "\n",
        "# สร้าง X_final ตาม selected_groups\n",
        "X_tr_final = pd.concat([feature_groups[g].loc[train_mask].copy() for g in selected_groups], axis=1)\n",
        "X_va_final = pd.concat([feature_groups[g].loc[valid_mask].copy() for g in selected_groups], axis=1)\n",
        "\n",
        "\n",
        "final_ranker, final_metrics = train_and_eval_ranker(\n",
        "    X_tr_final, y_tr, grp_tr,\n",
        "    X_va_final, y_va, grp_va,\n",
        "    base=feat_base, valid_mask=valid_mask,\n",
        "    eval_name='final_selected_groups'\n",
        ")\n",
        "print(final_metrics)\n",
        "\n",
        "vf = make_scored_frame(final_ranker, X_va_final, feat_base, valid_mask)\n",
        "for K in [1,3,5,10]:\n",
        "    r,p = recall_precision_at_k(vf, K)\n",
        "    hr  = hitrate_at_k(vf, K)\n",
        "    mrr = mrr_at_k(vf, K)\n",
        "    mAP = map_at_k(vf, K)\n",
        "    print(f\"K={K}  Recall={r:.4f}  Precision={p:.4f}  HitRate={hr:.4f}  MRR={mrr:.4f}  MAP={mAP:.4f}\")\n",
        "\n",
        "\n",
        "for c in X_tr_final.columns:\n",
        "    if X_tr_final[c].dtype == 'bool':\n",
        "        X_tr_final[c] = X_tr_final[c].astype(np.uint8)\n",
        "        X_va_final[c] = X_va_final[c].astype(np.uint8)\n",
        "\n",
        "print('Selected groups:', selected_groups)\n",
        "X_tr_final.shape, X_va_final.shape"
      ],
      "metadata": {
        "id": "GVFO4B3BjVMd"
      },
      "id": "GVFO4B3BjVMd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.Train LGBMRanker (LambdaRank)\n",
        "\n",
        "#### Purpose\n",
        "Train a gradient-boosted ranking model with LambdaRank objective and NDCG metric.\n",
        "\n",
        "#### Notes\n",
        "- Uses early stopping callbacks.\n",
        "- Consider `n_jobs=-1` to use all CPU cores.\n",
        "- Training groups provided via `grp_tr`/`grp_va`."
      ],
      "metadata": {
        "id": "CVefRJMHGouR"
      },
      "id": "CVefRJMHGouR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.Evaluation\n",
        "#### Purpose\n",
        "Compute Recall@K and Precision@K across queries by taking the top-K per `qid`.\n",
        "\n",
        "#### Outputs\n",
        "- Printed Recall@10/3/1 and Precision@10/3/1 for validation."
      ],
      "metadata": {
        "id": "OtABUZmoG3Ah"
      },
      "id": "OtABUZmoG3Ah"
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# SANITY CHECK 1: Random Label Test\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RUNNING SANITY CHECK: RANDOM LABEL TEST\")\n",
        "print(\"=\"*70)\n",
        "print(\"Expected: NDCG@10 should be ~0.10-0.25 (random baseline)\")\n",
        "print(\"If higher than 0.30 = model is memorizing or has leakage\\n\")\n",
        "\n",
        "from sklearn.metrics import ndcg_score\n",
        "\n",
        "random_ndcg = sanity_check_random_labels(\n",
        "    X_tr_final, y_tr, grp_tr,\n",
        "    X_va_final, y_va, grp_va\n",
        ")\n",
        "\n",
        "if random_ndcg < 0.30:\n",
        "    print(f\"\\n✅ PASS: Random NDCG = {random_ndcg:.4f} < 0.30\")\n",
        "else:\n",
        "    print(f\"\\n❌ FAIL: Random NDCG = {random_ndcg:.4f} >= 0.30 (too high!)\")"
      ],
      "metadata": {
        "id": "RsR-Vf3IHULg"
      },
      "id": "RsR-Vf3IHULg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# SANITY CHECK 2: Hold-out Saturday Test\n",
        "# ==========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"RUNNING SANITY CHECK: HOLD-OUT SATURDAY TEST\")\n",
        "print(\"=\"*70)\n",
        "print(\"Training WITHOUT Saturday, testing ON Saturday only\")\n",
        "print(\"Expected: NDCG@10 should be >0.25 if model doesn't overfit to DOW\\n\")\n",
        "\n",
        "# Concat train + valid\n",
        "X_all = pd.concat([X_tr_final, X_va_final], axis=0)\n",
        "y_all = np.concatenate([y_tr, y_va])\n",
        "\n",
        "# สร้าง feat_base_combined (เฉพาะแถว train + valid)\n",
        "combined_mask = train_mask | valid_mask\n",
        "feat_base_combined = feat_base.loc[combined_mask].copy()\n",
        "\n",
        "# เพิ่ม feat_dayofweek ถ้ายังไม่มี\n",
        "if 'feat_dayofweek' not in feat_base_combined.columns:\n",
        "    feat_base_combined['feat_dayofweek'] = feat_base_combined['day_t'].astype(int) % 7\n",
        "\n",
        "from sklearn.metrics import ndcg_score\n",
        "\n",
        "holdout_ndcg = sanity_check_holdout_dow(\n",
        "    feat_base_combined,  # ⚠️ ใช้ feat_base_combined แทน feat_base\n",
        "    X_all, y_all, grp_tr + grp_va,\n",
        "    dow_to_holdout=5  # Saturday\n",
        ")\n",
        "\n",
        "if holdout_ndcg > 0.30:\n",
        "    print(f\"\\n✅ PASS: Holdout NDCG = {holdout_ndcg:.4f} > 0.30 (generalizes well)\")\n",
        "elif holdout_ndcg > 0.15:\n",
        "    print(f\"\\n⚠️ WARNING: Holdout NDCG = {holdout_ndcg:.4f} (moderate DOW dependency)\")\n",
        "else:\n",
        "    print(f\"\\n❌ FAIL: Holdout NDCG = {holdout_ndcg:.4f} <= 0.15 (strong temporal leakage!)\")"
      ],
      "metadata": {
        "id": "P_s5LNrDHX1Z"
      },
      "id": "P_s5LNrDHX1Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# สร้าง valid_frame จาก pipeline ใหม่\n",
        "valid_frame = make_scored_frame(final_ranker, X_va_final, feat_base, valid_mask)\n",
        "\n",
        "# คำนวณ metrics\n",
        "for K in [1,3,5,10]:\n",
        "    r,p = recall_precision_at_k(valid_frame, K)\n",
        "    hr  = hitrate_at_k(valid_frame, K)\n",
        "    mrr = mrr_at_k(valid_frame, K)\n",
        "    mAP = map_at_k(valid_frame, K)\n",
        "    print(f\"K={K}  Recall={r:.4f}  Precision={p:.4f}  HitRate={hr:.4f}  MRR={mrr:.4f}  MAP={mAP:.4f}\")"
      ],
      "metadata": {
        "id": "XL-v-MgDF2pP"
      },
      "id": "XL-v-MgDF2pP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 10: simple Recall@K / Precision@K ===\n",
        "def recall_precision_at_k(df, k=10):\n",
        "    got = (df.sort_values([\"qid\",\"score\"], ascending=[True, False])\n",
        "             .groupby(\"qid\").head(k))\n",
        "    # recall@k = #positive ที่ดึงติด / #positive ทั้งหมดในกลุ่ม\n",
        "    pos_per_q = df.groupby(\"qid\")[\"label\"].sum()\n",
        "    hit_per_q = got.groupby(\"qid\")[\"label\"].sum()\n",
        "    recall = (hit_per_q / pos_per_q.replace(0, np.nan)).mean()  # เฉลี่ยเฉพาะกลุ่มที่มี positive\n",
        "    precision = got.groupby(\"qid\")[\"label\"].mean().mean()       # เฉลี่ย precision ต่อ qid\n",
        "    return recall, precision\n"
      ],
      "metadata": {
        "id": "CLKRzMLRF314"
      },
      "id": "CLKRzMLRF314",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build valid_frame from the new pipeline\n",
        "# Requires: final_ranker, X_va_final, feat_base, valid_mask, and the metric utils loaded\n",
        "valid_frame = make_scored_frame(final_ranker, X_va_final, feat_base, valid_mask)\n",
        "\n",
        "# Now compute metrics\n",
        "recall10, precision10 = recall_precision_at_k(valid_frame, k=10)\n",
        "print(f\"Recall@10={recall10:.4f}  Precision@10={precision10:.4f}\")\n",
        "\n",
        "recall3, precision3 = recall_precision_at_k(valid_frame, k=3)\n",
        "print(f\"Recall@3={recall3:.4f}  Precision@3={precision3:.4f}\")\n",
        "\n",
        "recall1, precision1 = recall_precision_at_k(valid_frame, k=1)\n",
        "print(f\"Recall@1={recall1:.4f}  Precision@1={precision1:.4f}\")"
      ],
      "metadata": {
        "id": "od0QeXmcyaxi"
      },
      "id": "od0QeXmcyaxi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ต้องมีตัวแปรเหล่านี้จาก pipeline ใหม่: final_ranker, X_va_final, feat_base, valid_mask\n",
        "# และมี promo_dept_set, promo_brand_set สำหรับบรรยายโปร\n",
        "\n",
        "valid_scored = feat_base.loc[valid_mask, ['household_key','BASKET_ID','PROMO_KEY','label','qid','day_t']].copy()\n",
        "valid_scored['score'] = final_ranker.predict(X_va_final)\n",
        "valid_scored['rank']  = valid_scored.groupby('qid')['score'].rank(ascending=False, method='first').astype(int)\n",
        "valid_scored['is_redeemed'] = valid_scored['label'].astype(int)\n",
        "\n",
        "# บรรยาย metadata ของโปร (optional)\n",
        "def _fmt_set(s):\n",
        "    ss = sorted(map(str, s)) if isinstance(s, set) else []\n",
        "    return ', '.join(ss[:5])  # ตัดให้สั้นอ่านง่าย\n",
        "valid_scored['promo_departments'] = valid_scored['PROMO_KEY'].map(lambda k: _fmt_set(promo_dept_set.get(k, set())))\n",
        "valid_scored['promo_brands']      = valid_scored['PROMO_KEY'].map(lambda k: _fmt_set(promo_brand_set.get(k, set())))\n",
        "\n",
        "# ดูตัวอย่าง 10 แถวแรก\n",
        "valid_scored.head(10)"
      ],
      "metadata": {
        "id": "wGvw7iQzxrdJ"
      },
      "id": "wGvw7iQzxrdJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Selected groups:', selected_groups)\n",
        "print('Num features:', X_tr_final.shape[1])\n",
        "\n",
        "# นับฟีเจอร์ต่อกลุ่ม\n",
        "for g in selected_groups:\n",
        "    print(g, feature_groups[g].shape[1])\n",
        "\n",
        "# รายชื่อคอลัมน์ฟีเจอร์ที่ใช้เทรน\n",
        "pd.Series(X_tr_final.columns, name='feature').to_frame().head(30)"
      ],
      "metadata": {
        "id": "ivmth8C3zYxF"
      },
      "id": "ivmth8C3zYxF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ใช้โมเดลสุดท้าย\n",
        "booster = final_ranker.booster_\n",
        "\n",
        "gain_imp  = pd.Series(booster.feature_importance(importance_type='gain'),  index=X_tr_final.columns, name='gain').sort_values(ascending=False)\n",
        "split_imp = pd.Series(booster.feature_importance(importance_type='split'), index=X_tr_final.columns, name='split').sort_values(ascending=False)\n",
        "\n",
        "display(gain_imp.head(30).to_frame())\n",
        "display(split_imp.head(30).to_frame())"
      ],
      "metadata": {
        "id": "5ovV1LiE0WlT"
      },
      "id": "5ovV1LiE0WlT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap, numpy as np\n",
        "\n",
        "# แปลง bool → uint8 เผื่อยังมีหลงเหลือ\n",
        "Xv = X_va_final.copy()\n",
        "for c in Xv.columns:\n",
        "    if Xv[c].dtype == bool:\n",
        "        Xv[c] = Xv[c].astype(np.uint8)\n",
        "\n",
        "sub_idx = np.random.RandomState(42).choice(len(Xv), size=min(4000, len(Xv)), replace=False)\n",
        "Xv_sub = Xv.iloc[sub_idx]\n",
        "\n",
        "explainer = shap.TreeExplainer(final_ranker.booster_)\n",
        "shap_values = explainer.shap_values(Xv_sub)\n",
        "\n",
        "shap_imp = pd.Series(np.abs(shap_values).mean(0), index=Xv_sub.columns, name='shap_mean_abs').sort_values(ascending=False)\n",
        "display(shap_imp.head(30).to_frame())\n",
        "\n"
      ],
      "metadata": {
        "id": "RWzsO7N40YD4"
      },
      "id": "RWzsO7N40YD4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns, matplotlib.pyplot as plt\n",
        "\n",
        "# ใช้เฉพาะ numeric และสุ่มลดขนาดเพื่อความเร็ว\n",
        "Xv_num = X_va_final.select_dtypes(include=[np.number])\n",
        "Xv_s = Xv_num.sample(n=min(8000, len(Xv_num)), random_state=42)\n",
        "\n",
        "corr = Xv_s.corr(method='spearman')\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.heatmap(corr.clip(-1,1), cmap='vlag', center=0)\n",
        "plt.title('Feature Spearman Correlation (validation subset)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GUatzt1J1oP6"
      },
      "id": "GUatzt1J1oP6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hitrate_at_k(df, k=10):\n",
        "    top = df.sort_values(['qid','score'], ascending=[True, False]).groupby('qid').head(k)\n",
        "    return (top.groupby('qid')['label'].max()).mean()\n",
        "\n",
        "def recall_precision_at_k(df, k=10):\n",
        "    got = (df.sort_values(['qid','score'], ascending=[True, False]).groupby('qid').head(k))\n",
        "    pos_per_q = df.groupby('qid')['label'].sum()\n",
        "    hit_per_q = got.groupby('qid')['label'].sum()\n",
        "    recall = (hit_per_q / pos_per_q.replace(0, np.nan)).mean()\n",
        "    precision = got.groupby('qid')['label'].mean().mean()\n",
        "    return recall, precision\n",
        "\n",
        "def mrr_at_k(df, k=10):\n",
        "    top = df.sort_values(['qid','score'], ascending=[True, False]).groupby('qid').head(k)\n",
        "    first_hit = top[top['label']==1].groupby('qid')['rank'].min()\n",
        "    return (1.0/first_hit).fillna(0).mean()\n",
        "\n",
        "def map_at_k(df, k=10):\n",
        "    tops = df.sort_values(['qid','score'], ascending=[True, False]).groupby('qid').head(k)\n",
        "    ap = []\n",
        "    for qid, sub in tops.groupby('qid'):\n",
        "        if sub['label'].sum() == 0:\n",
        "            continue\n",
        "        sub = sub.sort_values('rank')\n",
        "        cum_hits = sub['label'].cumsum()\n",
        "        prec_i = cum_hits / np.arange(1, len(sub)+1)\n",
        "        ap.append((prec_i * sub['label']).sum() / sub['label'].sum())\n",
        "    return np.mean(ap) if ap else 0.0\n",
        "\n",
        "for K in [1,3,5,10]:\n",
        "    r,p = recall_precision_at_k(valid_frame, k=K)\n",
        "    hr  = hitrate_at_k(valid_frame, k=K)\n",
        "    mrr = mrr_at_k(valid_frame, k=K)\n",
        "    mAP = map_at_k(valid_frame, k=K)\n",
        "    print(f\"K={K}  Recall={r:.4f}  Precision={p:.4f}  HitRate={hr:.4f}  MRR={mrr:.4f}  MAP={mAP:.4f}\")"
      ],
      "metadata": {
        "id": "Pex9eMaoaFZv"
      },
      "id": "Pex9eMaoaFZv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ndcg_score\n",
        "\n",
        "def ndcg_at_k_from_valid_frame(valid_frame, k=10):\n",
        "    ndcgs = []\n",
        "    for qid, g in valid_frame.groupby('qid'):\n",
        "        y_true = g['label'].to_numpy()\n",
        "        y_pred = g['score'].to_numpy()\n",
        "        if y_true.sum() == 0:\n",
        "            continue\n",
        "        ndcgs.append(ndcg_score(y_true.reshape(1,-1), y_pred.reshape(1,-1), k=k))\n",
        "    return float(np.mean(ndcgs)) if ndcgs else 0.0\n",
        "\n",
        "print('NDCG@5 =', ndcg_at_k_from_valid_frame(valid_frame, k=5))\n",
        "print('NDCG@10 =', ndcg_at_k_from_valid_frame(valid_frame, k=10))"
      ],
      "metadata": {
        "id": "QOlynBYH1n-m"
      },
      "id": "QOlynBYH1n-m",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}